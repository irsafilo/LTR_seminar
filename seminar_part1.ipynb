{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a520cda1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:09:38.434770Z",
     "start_time": "2021-11-10T18:09:38.193443Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from itertools import islice, cycle\n",
    "from more_itertools import pairwise\n",
    "import scipy.sparse as sp\n",
    "from itertools import islice, cycle\n",
    "#from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8642acaf",
   "metadata": {},
   "source": [
    "# Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93513960",
   "metadata": {},
   "source": [
    "Изучаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d45ef56c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:02.349609Z",
     "start_time": "2021-11-08T18:17:01.663644Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/interactions.csv')\n",
    "df_users = pd.read_csv('data/users.csv')\n",
    "df_items = pd.read_csv('data/items.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce53e994",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:02.368858Z",
     "start_time": "2021-11-08T18:17:02.353111Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>progress</th>\n",
       "      <th>rating</th>\n",
       "      <th>start_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126706</td>\n",
       "      <td>14433</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127290</td>\n",
       "      <td>140952</td>\n",
       "      <td>58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66991</td>\n",
       "      <td>198453</td>\n",
       "      <td>89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46791</td>\n",
       "      <td>83486</td>\n",
       "      <td>23</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79313</td>\n",
       "      <td>188770</td>\n",
       "      <td>88</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  progress  rating  start_date\n",
       "0   126706    14433        80     NaN  2018-01-01\n",
       "1   127290   140952        58     NaN  2018-01-01\n",
       "2    66991   198453        89     NaN  2018-01-01\n",
       "3    46791    83486        23     5.0  2018-01-01\n",
       "4    79313   188770        88     5.0  2018-01-01"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c302cff6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:04.618512Z",
     "start_time": "2021-11-08T18:17:04.472424Z"
    }
   },
   "outputs": [],
   "source": [
    "df['start_date'] = pd.to_datetime(df['start_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbca70f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:09.308618Z",
     "start_time": "2021-11-08T18:17:09.094599Z"
    }
   },
   "outputs": [],
   "source": [
    "duplicates = df.duplicated(subset=['user_id', 'item_id'], keep=False)\n",
    "df_duplicates = df[duplicates].sort_values(by=['user_id', 'start_date'])\n",
    "df = df[~duplicates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "653c2a48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:09.929391Z",
     "start_time": "2021-11-08T18:17:09.880048Z"
    }
   },
   "outputs": [],
   "source": [
    "df_duplicates = df_duplicates.groupby(['user_id', 'item_id']).agg({\n",
    "    'progress': 'max',\n",
    "    'rating': 'max',\n",
    "    'start_date': 'min'\n",
    "})\n",
    "df = df.append(df_duplicates.reset_index(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a79cadd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:10.763103Z",
     "start_time": "2021-11-08T18:17:10.739050Z"
    }
   },
   "outputs": [],
   "source": [
    "df['progress'] = df['progress'].astype(np.int8)\n",
    "df['rating'] = df['rating'].astype(pd.SparseDtype(np.float32, np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4593d138",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:16.012617Z",
     "start_time": "2021-11-08T18:17:15.927161Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_pickle('data/interactions_preprocessed.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9d38a09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:17.023782Z",
     "start_time": "2021-11-08T18:17:17.012940Z"
    }
   },
   "outputs": [],
   "source": [
    "df_users['age'] = df_users['age'].astype('category')\n",
    "df_users['sex'] = df_users['sex'].astype(pd.SparseDtype(np.float32, np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cd1e5cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:35.257635Z",
     "start_time": "2021-11-08T18:17:35.188151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кол-во пользователей - 158811\n",
      "Кол-во пользователей c взаимодействиями и фичами - 135677 (85.43%)\n",
      "Кол-во пользователей только c взаимодействиями - 15923 (10.03%)\n",
      "Кол-во пользователей только c фичами - 7211 (4.54%)\n"
     ]
    }
   ],
   "source": [
    "interaction_users = df['user_id'].unique()\n",
    "\n",
    "common_users = len(np.intersect1d(interaction_users, df_users['user_id']))\n",
    "users_only_in_interaction = len(np.setdiff1d(interaction_users, df_users['user_id']))\n",
    "users_only_features = len(np.setdiff1d(df_users['user_id'], interaction_users))\n",
    "total_users = common_users + users_only_in_interaction + users_only_features\n",
    "print(f'Кол-во пользователей - {total_users}')\n",
    "print(f'Кол-во пользователей c взаимодействиями и фичами - {common_users} ({common_users / total_users * 100:.2f}%)')\n",
    "print(f'Кол-во пользователей только c взаимодействиями - {users_only_in_interaction} ({users_only_in_interaction / total_users * 100:.2f}%)')\n",
    "print(f'Кол-во пользователей только c фичами - {users_only_features} ({users_only_features / total_users * 100:.2f}%)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fc3a6f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:36.255029Z",
     "start_time": "2021-11-08T18:17:36.247882Z"
    }
   },
   "outputs": [],
   "source": [
    "df_users.to_pickle('data/users_preprocessed.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9854565",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:36.708589Z",
     "start_time": "2021-11-08T18:17:36.656778Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in ['genres', 'authors', 'year']:\n",
    "    df_items[col] = df_items[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8780bfbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:36.931834Z",
     "start_time": "2021-11-08T18:17:36.879885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кол-во книг - 59599\n",
      "Кол-во книг c взаимодействиями и фичами - 59599 (100.00%)\n",
      "Кол-во книг только c взаимодействиями - 0 (0.00%)\n",
      "Кол-во книг только c фичами - 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "interaction_items = df['item_id'].unique()\n",
    "\n",
    "common_items = len(np.intersect1d(interaction_items, df_items['id']))\n",
    "items_only_in_interaction = len(np.setdiff1d(interaction_items, df_items['id']))\n",
    "items_only_features = len(np.setdiff1d(df_items['id'], interaction_items))\n",
    "total_items = common_items + items_only_in_interaction + items_only_features\n",
    "print(f'Кол-во книг - {total_items}')\n",
    "print(f'Кол-во книг c взаимодействиями и фичами - {common_items} ({common_items / total_items * 100:.2f}%)')\n",
    "print(f'Кол-во книг только c взаимодействиями - {items_only_in_interaction} ({items_only_in_interaction / total_items * 100:.2f}%)')\n",
    "print(f'Кол-во книг только c фичами - {items_only_features} ({items_only_features / total_items * 100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5db0b44b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:39.712615Z",
     "start_time": "2021-11-08T18:17:39.667874Z"
    }
   },
   "outputs": [],
   "source": [
    "df_items.to_pickle('data/items_preprocessed.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0202a3a4",
   "metadata": {},
   "source": [
    "# Метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96fbe1ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:40.486626Z",
     "start_time": "2021-11-08T18:17:40.479257Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Аня</td>\n",
       "      <td>Мастер и Маргарита</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Боря</td>\n",
       "      <td>451° по Фаренгейту</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Вася</td>\n",
       "      <td>Зеленая миля</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Вася</td>\n",
       "      <td>Рита Хейуорт и спасение из Шоушенка</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id                              item_id\n",
       "0     Аня                   Мастер и Маргарита\n",
       "1    Боря                   451° по Фаренгейту\n",
       "2    Вася                         Зеленая миля\n",
       "3    Вася  Рита Хейуорт и спасение из Шоушенка"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_true = pd.DataFrame({\n",
    "    'user_id': ['Аня',                'Боря',               'Вася',         'Вася'],\n",
    "    'item_id': ['Мастер и Маргарита', '451° по Фаренгейту', 'Зеленая миля', 'Рита Хейуорт и спасение из Шоушенка'],\n",
    "})\n",
    "df_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fd989d",
   "metadata": {},
   "source": [
    "Precision@k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30897d7",
   "metadata": {},
   "source": [
    "Вначале посчитаем метрик для топ-2 (т.е. К = 2). Алгоритм следующий:\n",
    "\n",
    "- Релевантные объекты, которые не были рекомендованы игнорируем (NaN)\n",
    "- Определяем, какие релеватные рекомендации попали в топ-2 (hit)\n",
    "- True positive для каждого пользователя\n",
    "Делим TP на K\n",
    "- Считаем Precision@K для каждого пользователя как сумму его TP/K\n",
    "- Все Precision@K усредняем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0e79455",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:42.733290Z",
     "start_time": "2021-11-08T18:17:42.725794Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Аня</td>\n",
       "      <td>Отверженные</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Аня</td>\n",
       "      <td>Двенадцать стульев</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Аня</td>\n",
       "      <td>Герои нашего времени</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Боря</td>\n",
       "      <td>451° по Фаренгейту</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Боря</td>\n",
       "      <td>1984</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Боря</td>\n",
       "      <td>О дивный новый мир</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Вася</td>\n",
       "      <td>Десять негритят</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Вася</td>\n",
       "      <td>Искра жизни</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Вася</td>\n",
       "      <td>Зеленая миля</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id               item_id  rank\n",
       "0     Аня           Отверженные     1\n",
       "1     Аня    Двенадцать стульев     2\n",
       "2     Аня  Герои нашего времени     3\n",
       "3    Боря    451° по Фаренгейту     1\n",
       "4    Боря                  1984     2\n",
       "5    Боря    О дивный новый мир     3\n",
       "6    Вася       Десять негритят     1\n",
       "7    Вася           Искра жизни     2\n",
       "8    Вася          Зеленая миля     3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_recs = pd.DataFrame({\n",
    "    'user_id': [\n",
    "        'Аня', 'Аня', 'Аня', \n",
    "        'Боря', 'Боря', 'Боря', \n",
    "        'Вася', 'Вася', 'Вася',\n",
    "    ],\n",
    "    'item_id': [\n",
    "        'Отверженные', 'Двенадцать стульев', 'Герои нашего времени', \n",
    "        '451° по Фаренгейту', '1984', 'О дивный новый мир',\n",
    "        'Десять негритят', 'Искра жизни', 'Зеленая миля', \n",
    "    ],\n",
    "    'rank': [\n",
    "        1, 2, 3,\n",
    "        1, 2, 3,\n",
    "        1, 2, 3,\n",
    "    ]\n",
    "})\n",
    "df_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c477af0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:43.897299Z",
     "start_time": "2021-11-08T18:17:43.887493Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Аня</th>\n",
       "      <th>Мастер и Маргарита</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Боря</th>\n",
       "      <th>451° по Фаренгейту</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Вася</th>\n",
       "      <th>Зеленая миля</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Рита Хейуорт и спасение из Шоушенка</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             rank\n",
       "user_id item_id                                  \n",
       "Аня     Мастер и Маргарита                    NaN\n",
       "Боря    451° по Фаренгейту                    1.0\n",
       "Вася    Зеленая миля                          3.0\n",
       "        Рита Хейуорт и спасение из Шоушенка   NaN"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged = df_true.set_index(['user_id', 'item_id']).join(df_recs.set_index(['user_id', 'item_id']), how='left')\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cd20fdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:45.166349Z",
     "start_time": "2021-11-08T18:17:45.159202Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>hit@2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Аня</th>\n",
       "      <th>Мастер и Маргарита</th>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Боря</th>\n",
       "      <th>451° по Фаренгейту</th>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Вася</th>\n",
       "      <th>Зеленая миля</th>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Рита Хейуорт и спасение из Шоушенка</th>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             rank  hit@2\n",
       "user_id item_id                                         \n",
       "Аня     Мастер и Маргарита                    NaN  False\n",
       "Боря    451° по Фаренгейту                    1.0   True\n",
       "Вася    Зеленая миля                          3.0  False\n",
       "        Рита Хейуорт и спасение из Шоушенка   NaN  False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged['hit@2'] = df_merged['rank'] <= 2\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daef8f9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:48.051412Z",
     "start_time": "2021-11-08T18:17:48.044813Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>hit@2</th>\n",
       "      <th>hit@2/2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Аня</th>\n",
       "      <th>Мастер и Маргарита</th>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Боря</th>\n",
       "      <th>451° по Фаренгейту</th>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Вася</th>\n",
       "      <th>Зеленая миля</th>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Рита Хейуорт и спасение из Шоушенка</th>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             rank  hit@2  hit@2/2\n",
       "user_id item_id                                                  \n",
       "Аня     Мастер и Маргарита                    NaN  False      0.0\n",
       "Боря    451° по Фаренгейту                    1.0   True      0.5\n",
       "Вася    Зеленая миля                          3.0  False      0.0\n",
       "        Рита Хейуорт и спасение из Шоушенка   NaN  False      0.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged['hit@2/2'] = df_merged['hit@2'] / 2\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c1c6e27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:48.400944Z",
     "start_time": "2021-11-08T18:17:48.396508Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "Аня     0.0\n",
       "Боря    0.5\n",
       "Вася    0.0\n",
       "Name: hit@2/2, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prec2 = df_merged.groupby(level=0)['hit@2/2'].sum()\n",
    "df_prec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d0e217b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:48.616171Z",
     "start_time": "2021-11-08T18:17:48.609581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@1 = 0.3333\n",
      "Precision@2 = 0.1667\n",
      "Precision@3 = 0.2222\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "users_count = df_merged.index.get_level_values('user_id').nunique()\n",
    "for k in [1, 2, 3]:\n",
    "    hit_k = f'hit@{k}'\n",
    "    df_merged[hit_k] = df_merged['rank'] <= k\n",
    "    print(f'Precision@{k} = {(df_merged[hit_k] / k).sum() / users_count:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5020a5",
   "metadata": {},
   "source": [
    "Recall@k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9fdf97",
   "metadata": {},
   "source": [
    "Посчитайте метрику полноты для k = 1, 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b36e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e1d95a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab8d24e4",
   "metadata": {},
   "source": [
    "Почему Precision@k и Recall@k не всегда хорошо считать?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbcc299",
   "metadata": {},
   "source": [
    "MRR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b9b75b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:53:22.060263Z",
     "start_time": "2021-11-08T18:53:22.056800Z"
    }
   },
   "source": [
    "Эта метрика оценивает качество топ-N рекомендаций c учетом рангов/позиций. Основная идея - оценить \"попадания\" с весом, зависящим от позиции (обычно это обратная пропорциальная зависимость, то есть чем больше позиция, тем меньше вес)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a520ad1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:55:13.371015Z",
     "start_time": "2021-11-08T18:55:13.368109Z"
    }
   },
   "source": [
    "**Mean Reciproal Rank, MRR**\n",
    "\n",
    "\n",
    "$MRR \\equiv \\frac{1}{Q} \\sum_{i=1}^Q\\frac{1}{r_i}$\n",
    "\n",
    "Где Q - это query или наш пользователь, а rank_i - позиция первой релевантной рекомендации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0fa5b3a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:56:58.943666Z",
     "start_time": "2021-11-08T18:56:58.935634Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Аня</td>\n",
       "      <td>Мастер и Маргарита</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Боря</td>\n",
       "      <td>451° по Фаренгейту</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Вася</td>\n",
       "      <td>Зеленая миля</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Вася</td>\n",
       "      <td>Рита Хейуорт и спасение из Шоушенка</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id                              item_id\n",
       "0     Аня                   Мастер и Маргарита\n",
       "1    Боря                   451° по Фаренгейту\n",
       "2    Вася                         Зеленая миля\n",
       "3    Вася  Рита Хейуорт и спасение из Шоушенка"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_true = pd.DataFrame({\n",
    "    'user_id': ['Аня',                'Боря',               'Вася',         'Вася'],\n",
    "    'item_id': ['Мастер и Маргарита', '451° по Фаренгейту', 'Зеленая миля', 'Рита Хейуорт и спасение из Шоушенка'],\n",
    "})\n",
    "df_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3a065759",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:56:59.331349Z",
     "start_time": "2021-11-08T18:56:59.324292Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Аня</td>\n",
       "      <td>Отверженные</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Аня</td>\n",
       "      <td>Двенадцать стульев</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Аня</td>\n",
       "      <td>Герои нашего времени</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Боря</td>\n",
       "      <td>451° по Фаренгейту</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Боря</td>\n",
       "      <td>1984</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Боря</td>\n",
       "      <td>О дивный новый мир</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Вася</td>\n",
       "      <td>Десять негритят</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Вася</td>\n",
       "      <td>Рита Хейуорт и спасение из Шоушенка</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Вася</td>\n",
       "      <td>Зеленая миля</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id                              item_id  rank\n",
       "0     Аня                          Отверженные     1\n",
       "1     Аня                   Двенадцать стульев     2\n",
       "2     Аня                 Герои нашего времени     3\n",
       "3    Боря                   451° по Фаренгейту     1\n",
       "4    Боря                                 1984     2\n",
       "5    Боря                   О дивный новый мир     3\n",
       "6    Вася                      Десять негритят     1\n",
       "7    Вася  Рита Хейуорт и спасение из Шоушенка     2\n",
       "8    Вася                         Зеленая миля     3"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_recs = pd.DataFrame({\n",
    "    'user_id': [\n",
    "        'Аня', 'Аня', 'Аня', \n",
    "        'Боря', 'Боря', 'Боря', \n",
    "        'Вася', 'Вася', 'Вася',\n",
    "    ],\n",
    "    'item_id': [\n",
    "        'Отверженные', 'Двенадцать стульев', 'Герои нашего времени', \n",
    "        '451° по Фаренгейту', '1984', 'О дивный новый мир',\n",
    "        'Десять негритят', 'Рита Хейуорт и спасение из Шоушенка', 'Зеленая миля', \n",
    "    ],\n",
    "    'rank': [\n",
    "        1, 2, 3,\n",
    "        1, 2, 3,\n",
    "        1, 2, 3,\n",
    "    ]\n",
    "})\n",
    "df_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3f4ea54a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:56:59.462036Z",
     "start_time": "2021-11-08T18:56:59.450989Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Аня</th>\n",
       "      <th>Мастер и Маргарита</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Боря</th>\n",
       "      <th>451° по Фаренгейту</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Вася</th>\n",
       "      <th>Рита Хейуорт и спасение из Шоушенка</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Зеленая миля</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             rank\n",
       "user_id item_id                                  \n",
       "Аня     Мастер и Маргарита                    NaN\n",
       "Боря    451° по Фаренгейту                    1.0\n",
       "Вася    Рита Хейуорт и спасение из Шоушенка   2.0\n",
       "        Зеленая миля                          3.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged = df_true.set_index(['user_id', 'item_id']).join(df_recs.set_index(['user_id', 'item_id']), how='left')\n",
    "df_merged = df_merged.sort_values(by=['user_id', 'rank'])\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fc78c02c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:56:59.677400Z",
     "start_time": "2021-11-08T18:56:59.669681Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>reciprocal_rank</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Аня</th>\n",
       "      <th>Мастер и Маргарита</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Боря</th>\n",
       "      <th>451° по Фаренгейту</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Вася</th>\n",
       "      <th>Рита Хейуорт и спасение из Шоушенка</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Зеленая миля</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             rank  reciprocal_rank\n",
       "user_id item_id                                                   \n",
       "Аня     Мастер и Маргарита                    NaN              NaN\n",
       "Боря    451° по Фаренгейту                    1.0         1.000000\n",
       "Вася    Рита Хейуорт и спасение из Шоушенка   2.0         0.500000\n",
       "        Зеленая миля                          3.0         0.333333"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged['reciprocal_rank'] = 1 / df_merged['rank']\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0f2a72f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:57:00.156710Z",
     "start_time": "2021-11-08T18:57:00.151133Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "Аня     NaN\n",
       "Боря    1.0\n",
       "Вася    0.5\n",
       "Name: reciprocal_rank, dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrr = df_merged.groupby(level='user_id')['reciprocal_rank'].max()\n",
    "mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "13732c53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:57:01.163513Z",
     "start_time": "2021-11-08T18:57:01.160955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR = 0.5\n"
     ]
    }
   ],
   "source": [
    "print(f\"MRR = {mrr.fillna(0).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a879d1",
   "metadata": {},
   "source": [
    "MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69493c3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T09:58:03.244956Z",
     "start_time": "2021-10-16T09:58:03.241920Z"
    }
   },
   "source": [
    "Написать код подсчета метрики MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f47d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f7b9aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04cc41cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T10:04:30.121897Z",
     "start_time": "2021-10-16T10:04:30.118865Z"
    }
   },
   "source": [
    "Посчитаем все на нашем датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "299b712c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:53.451834Z",
     "start_time": "2021-11-08T18:17:53.442151Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(numpy.datetime64('2019-12-25T00:00:00.000000000'),\n",
       "  numpy.datetime64('2019-12-26T00:00:00.000000000')),\n",
       " (numpy.datetime64('2019-12-26T00:00:00.000000000'),\n",
       "  numpy.datetime64('2019-12-27T00:00:00.000000000')),\n",
       " (numpy.datetime64('2019-12-27T00:00:00.000000000'),\n",
       "  numpy.datetime64('2019-12-28T00:00:00.000000000')),\n",
       " (numpy.datetime64('2019-12-28T00:00:00.000000000'),\n",
       "  numpy.datetime64('2019-12-29T00:00:00.000000000')),\n",
       " (numpy.datetime64('2019-12-29T00:00:00.000000000'),\n",
       "  numpy.datetime64('2019-12-30T00:00:00.000000000')),\n",
       " (numpy.datetime64('2019-12-30T00:00:00.000000000'),\n",
       "  numpy.datetime64('2019-12-31T00:00:00.000000000'))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dates = df['start_date'].unique()[-7:]\n",
    "test_dates = list(pairwise(test_dates))\n",
    "test_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9675b00d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:53.791201Z",
     "start_time": "2021-11-08T18:17:53.633233Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((numpy.datetime64('2019-12-25T00:00:00.000000000'),\n",
       "  numpy.datetime64('2019-12-26T00:00:00.000000000')),\n",
       " (1517994, 5),\n",
       " (2114, 5))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dates = test_dates[0]\n",
    "train = df[df['start_date'] < split_dates[0]]\n",
    "test = df[(df['start_date'] >= split_dates[0]) & (df['start_date'] < split_dates[1])]\n",
    "test = test[(test['rating'] >= 4) | (test['rating'].isnull())]\n",
    "split_dates, train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0139a2a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:53.860365Z",
     "start_time": "2021-11-08T18:17:53.855841Z"
    }
   },
   "outputs": [],
   "source": [
    "class PopularRecommender():\n",
    "    def __init__(self, max_K=100, days=30, item_column='item_id', dt_column='date'):\n",
    "        self.max_K = max_K\n",
    "        self.days = days\n",
    "        self.item_column = item_column\n",
    "        self.dt_column = dt_column\n",
    "        self.recommendations = []\n",
    "        \n",
    "    def fit(self, df, ):\n",
    "        min_date = df[self.dt_column].max().normalize() - pd.DateOffset(days=self.days)\n",
    "        self.recommendations = df.loc[df[self.dt_column] > min_date, self.item_column].value_counts().head(self.max_K).index.values\n",
    "    \n",
    "    def recommend(self, users=None, N=10):\n",
    "        recs = self.recommendations[:N]\n",
    "        if users is None:\n",
    "            return recs\n",
    "        else:\n",
    "            return list(islice(cycle([recs]), len(users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08e128e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:54.078761Z",
     "start_time": "2021-11-08T18:17:54.068340Z"
    }
   },
   "outputs": [],
   "source": [
    "pop_model = PopularRecommender(days=7, dt_column='start_date')\n",
    "pop_model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8fafaf74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:54.256571Z",
     "start_time": "2021-11-08T18:17:54.254662Z"
    }
   },
   "outputs": [],
   "source": [
    "top10_recs = pop_model.recommend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7061b3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:54.762427Z",
     "start_time": "2021-11-08T18:17:54.739310Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ворон-челобитчик'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_titles = pd.Series(df_items['title'].values, index=df_items['id']).to_dict()\n",
    "item_titles[128115]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "178765a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:54.946859Z",
     "start_time": "2021-11-08T18:17:54.943465Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Пикник на обочине',\n",
       " 'Кавказский пленник',\n",
       " 'Записки юного врача',\n",
       " 'Богатый папа, бедный папа. Роберт Кийосаки (обзор)',\n",
       " 'Москва и москвичи',\n",
       " 'Понедельник начинается в субботу',\n",
       " 'Хитрость',\n",
       " 'Сказка о попе и о работнике его Балде',\n",
       " 'Лорд, который влюбился. Тайный жених',\n",
       " 'История государства Российского. Том 2. От Великого князя Святополка до Великого князя Мстислава Изяславовича']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(item_titles.get, top10_recs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6020f58c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:55.173902Z",
     "start_time": "2021-11-08T18:17:55.164747Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38753</td>\n",
       "      <td>[235407, 230067, 35265, 281005, 147734, 208935...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101642</td>\n",
       "      <td>[235407, 230067, 35265, 281005, 147734, 208935...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13548</td>\n",
       "      <td>[235407, 230067, 35265, 281005, 147734, 208935...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>130425</td>\n",
       "      <td>[235407, 230067, 35265, 281005, 147734, 208935...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93986</td>\n",
       "      <td>[235407, 230067, 35265, 281005, 147734, 208935...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                            item_id\n",
       "0    38753  [235407, 230067, 35265, 281005, 147734, 208935...\n",
       "1   101642  [235407, 230067, 35265, 281005, 147734, 208935...\n",
       "2    13548  [235407, 230067, 35265, 281005, 147734, 208935...\n",
       "3   130425  [235407, 230067, 35265, 281005, 147734, 208935...\n",
       "4    93986  [235407, 230067, 35265, 281005, 147734, 208935..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recs = pd.DataFrame({'user_id': test['user_id'].unique()})\n",
    "top_N = 10\n",
    "recs['item_id'] = pop_model.recommend(recs['user_id'], N=top_N)\n",
    "recs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7f54f9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:55.400525Z",
     "start_time": "2021-11-08T18:17:55.388913Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38753</td>\n",
       "      <td>235407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38753</td>\n",
       "      <td>230067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38753</td>\n",
       "      <td>35265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38753</td>\n",
       "      <td>281005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38753</td>\n",
       "      <td>147734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38753</td>\n",
       "      <td>208935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38753</td>\n",
       "      <td>285394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38753</td>\n",
       "      <td>96052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38753</td>\n",
       "      <td>62715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38753</td>\n",
       "      <td>151190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101642</td>\n",
       "      <td>235407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101642</td>\n",
       "      <td>230067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id item_id\n",
       "0    38753  235407\n",
       "0    38753  230067\n",
       "0    38753   35265\n",
       "0    38753  281005\n",
       "0    38753  147734\n",
       "0    38753  208935\n",
       "0    38753  285394\n",
       "0    38753   96052\n",
       "0    38753   62715\n",
       "0    38753  151190\n",
       "1   101642  235407\n",
       "1   101642  230067"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recs = recs.explode('item_id')\n",
    "recs.head(top_N + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd2c6eb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:55.812743Z",
     "start_time": "2021-11-08T18:17:55.804334Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38753</td>\n",
       "      <td>235407</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38753</td>\n",
       "      <td>230067</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38753</td>\n",
       "      <td>35265</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38753</td>\n",
       "      <td>281005</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38753</td>\n",
       "      <td>147734</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38753</td>\n",
       "      <td>208935</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38753</td>\n",
       "      <td>285394</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38753</td>\n",
       "      <td>96052</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38753</td>\n",
       "      <td>62715</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38753</td>\n",
       "      <td>151190</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101642</td>\n",
       "      <td>235407</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101642</td>\n",
       "      <td>230067</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id item_id  rank\n",
       "0    38753  235407     1\n",
       "0    38753  230067     2\n",
       "0    38753   35265     3\n",
       "0    38753  281005     4\n",
       "0    38753  147734     5\n",
       "0    38753  208935     6\n",
       "0    38753  285394     7\n",
       "0    38753   96052     8\n",
       "0    38753   62715     9\n",
       "0    38753  151190    10\n",
       "1   101642  235407     1\n",
       "1   101642  230067     2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recs['rank'] = recs.groupby('user_id').cumcount() + 1\n",
    "recs.head(top_N + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb3ecb04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:56.167483Z",
     "start_time": "2021-11-08T18:17:56.164061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id     int64\n",
       "item_id    object\n",
       "rank        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recs.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22663fff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:56.544377Z",
     "start_time": "2021-11-08T18:17:56.541776Z"
    }
   },
   "outputs": [],
   "source": [
    "recs['item_id'] = recs['item_id'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "623edda4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:56.825707Z",
     "start_time": "2021-11-08T18:17:56.819828Z"
    }
   },
   "outputs": [],
   "source": [
    "test_recs = test.merge(recs, on = ['user_id', 'item_id'], how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc9279ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:57.146892Z",
     "start_time": "2021-11-08T18:17:57.139137Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>progress</th>\n",
       "      <th>rating</th>\n",
       "      <th>start_date</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102560</td>\n",
       "      <td>147734</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-12-25</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33152</td>\n",
       "      <td>147734</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-12-25</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55393</td>\n",
       "      <td>147734</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-12-25</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>129781</td>\n",
       "      <td>147734</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-12-25</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57724</td>\n",
       "      <td>230067</td>\n",
       "      <td>74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-12-25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  progress  rating start_date  rank\n",
       "0   102560   147734         0     NaN 2019-12-25     5\n",
       "1    33152   147734         0     NaN 2019-12-25     5\n",
       "2    55393   147734         1     NaN 2019-12-25     5\n",
       "3   129781   147734         1     NaN 2019-12-25     5\n",
       "4    57724   230067        74     NaN 2019-12-25     2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_recs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2c0b820b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:57.367523Z",
     "start_time": "2021-11-08T18:17:57.358186Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>progress</th>\n",
       "      <th>rating</th>\n",
       "      <th>start_date</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>474</td>\n",
       "      <td>235407</td>\n",
       "      <td>100</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2019-12-25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1672</td>\n",
       "      <td>230067</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-12-25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2345</td>\n",
       "      <td>208935</td>\n",
       "      <td>76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-12-25</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>9279</td>\n",
       "      <td>96052</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-12-25</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>10260</td>\n",
       "      <td>35265</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-12-25</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  item_id  progress  rating start_date  rank\n",
       "46      474   235407       100     5.0 2019-12-25     1\n",
       "18     1672   230067        12     NaN 2019-12-25     2\n",
       "25     2345   208935        76     NaN 2019-12-25     6\n",
       "61     9279    96052       100     NaN 2019-12-25     8\n",
       "52    10260    35265         0     NaN 2019-12-25     3"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_recs = test_recs.sort_values(by=['user_id', 'rank'])\n",
    "test_recs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c783a855",
   "metadata": {},
   "source": [
    "### NDCG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503845fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T10:10:17.967565Z",
     "start_time": "2021-10-16T10:10:17.964807Z"
    }
   },
   "source": [
    "Посчитайте метрику ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932d75c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb285a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "97daada1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:17:58.932157Z",
     "start_time": "2021-11-08T18:17:58.659263Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>progress</th>\n",
       "      <th>rating</th>\n",
       "      <th>start_date</th>\n",
       "      <th>rank</th>\n",
       "      <th>users_item_count</th>\n",
       "      <th>reciprocal_rank</th>\n",
       "      <th>cumulative_rank</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129222</th>\n",
       "      <th>275441</th>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-12-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18067</th>\n",
       "      <th>69599</th>\n",
       "      <td>77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-12-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76378</th>\n",
       "      <th>246837</th>\n",
       "      <td>93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-12-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135722</th>\n",
       "      <th>267580</th>\n",
       "      <td>71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-12-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92498</th>\n",
       "      <th>39822</th>\n",
       "      <td>35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-12-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 progress  rating start_date  rank  users_item_count  \\\n",
       "user_id item_id                                                        \n",
       "129222  275441         24     NaN 2019-12-25   NaN               1.0   \n",
       "18067   69599          77     NaN 2019-12-25   NaN               1.0   \n",
       "76378   246837         93     NaN 2019-12-25   NaN               1.0   \n",
       "135722  267580         71     NaN 2019-12-25   NaN               1.0   \n",
       "92498   39822          35     NaN 2019-12-25   NaN               1.0   \n",
       "\n",
       "                 reciprocal_rank  cumulative_rank  \n",
       "user_id item_id                                    \n",
       "129222  275441               0.0              NaN  \n",
       "18067   69599                0.0              NaN  \n",
       "76378   246837               0.0              NaN  \n",
       "135722  267580               0.0              NaN  \n",
       "92498   39822                0.0              NaN  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_recs = test.set_index(['user_id', 'item_id']).join(recs.set_index(['user_id', 'item_id']))\n",
    "test_recs['users_item_count'] = test_recs.groupby(level='user_id', sort=False)['rank'].transform(np.size)\n",
    "test_recs['reciprocal_rank'] = 1 / test_recs['rank']\n",
    "test_recs['reciprocal_rank'] = test_recs['reciprocal_rank'].fillna(0)\n",
    "test_recs['cumulative_rank'] = test_recs.groupby(level='user_id').cumcount() + 1\n",
    "test_recs['cumulative_rank'] = test_recs['cumulative_rank'] / test_recs['rank']\n",
    "test_recs.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704f160f",
   "metadata": {},
   "source": [
    "Посчитайте метрики по test (2019-12-25, 2019-12-26)\n",
    "Precision@1 = \n",
    "\n",
    "Recall@1 = \n",
    "\n",
    "Precision@2 = \n",
    "\n",
    "Recall@2 = \n",
    "\n",
    "Precision@3 = \n",
    "\n",
    "Recall@3 = \n",
    "\n",
    "Precision@4 = \n",
    "\n",
    "Recall@4 = \n",
    "\n",
    "Precision@5 = \n",
    "\n",
    "Recall@5 = \n",
    "\n",
    "Precision@6 = \n",
    "\n",
    "Recall@6 = \n",
    "\n",
    "Precision@7 = \n",
    "\n",
    "Recall@7 = \n",
    "\n",
    "Precision@8 = \n",
    "\n",
    "Recall@8 = \n",
    "\n",
    "Precision@9 =\n",
    "\n",
    "Recall@9 = \n",
    "\n",
    "Precision@10 = \n",
    "\n",
    "Recall@10 = \n",
    "\n",
    "MAP@10 = \n",
    "\n",
    "MRR = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688c8496",
   "metadata": {},
   "source": [
    "# ItemToItem модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24273f38",
   "metadata": {},
   "source": [
    "### Нужно запрограммировать BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a3c50994",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:17:05.606969Z",
     "start_time": "2021-11-08T19:17:05.595588Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class BM25:\n",
    "    def __init__(self, corpus, tokenizer=None):\n",
    "        self.corpus_size = len(corpus)\n",
    "        self.avgdl = 0\n",
    "        self.doc_freqs = []\n",
    "        self.idf = {}\n",
    "        self.doc_len = []\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        if tokenizer:\n",
    "            corpus = self._tokenize_corpus(corpus)\n",
    "\n",
    "        nd = self._initialize(corpus)\n",
    "        self._calc_idf(nd)\n",
    "\n",
    "    def _initialize(self, corpus):\n",
    "        nd = {}  # word -> number of documents with word\n",
    "        num_doc = 0\n",
    "        for document in corpus:\n",
    "            self.doc_len.append(len(document))\n",
    "            num_doc += len(document)\n",
    "\n",
    "            frequencies = {}\n",
    "            for word in document:\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 0\n",
    "                frequencies[word] += 1\n",
    "            self.doc_freqs.append(frequencies)\n",
    "\n",
    "            for word, freq in frequencies.items():\n",
    "                try:\n",
    "                    nd[word]+=1\n",
    "                except KeyError:\n",
    "                    nd[word] = 1\n",
    "\n",
    "        self.avgdl = num_doc / self.corpus_size\n",
    "        return nd\n",
    "\n",
    "    def _tokenize_corpus(self, corpus):\n",
    "        pool = Pool(cpu_count())\n",
    "        tokenized_corpus = pool.map(self.tokenizer, corpus)\n",
    "        return tokenized_corpus\n",
    "\n",
    "    def _calc_idf(self, nd):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_scores(self, query):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_batch_scores(self, query, doc_ids):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_top_n(self, query, documents, n=5):\n",
    "\n",
    "        assert self.corpus_size == len(documents), \"The documents given don't match the index corpus!\"\n",
    "\n",
    "        scores = self.get_scores(query)\n",
    "        top_n = np.argsort(scores)[::-1][:n]\n",
    "        return [documents[i] for i in top_n]\n",
    "\n",
    "\n",
    "class BM25Okapi(BM25):\n",
    "    def __init__(self, corpus, tokenizer=None, k1=1.5, b=0.75, epsilon=0.25):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.epsilon = epsilon\n",
    "        super().__init__(corpus, tokenizer)\n",
    "\n",
    "    def _calc_idf(self, nd):\n",
    "        \"\"\"\n",
    "        Calculates frequencies of terms in documents and in corpus.\n",
    "        This algorithm sets a floor on the idf values to eps * average_idf\n",
    "        \"\"\"\n",
    "        # collect idf sum to calculate an average idf for epsilon value\n",
    "        idf_sum = 0\n",
    "        # collect words with negative idf to set them a special epsilon value.\n",
    "        # idf can be negative if word is contained in more than half of documents\n",
    "\n",
    "    def get_scores(self, query):\n",
    "        \"\"\"\n",
    "        The ATIRE BM25 variant uses an idf function which uses a log(idf) score. To prevent negative idf scores,\n",
    "        this algorithm also adds a floor to the idf value of epsilon.\n",
    "        See [Trotman, A., X. Jia, M. Crane, Towards an Efficient and Effective Search Engine] for more info\n",
    "        :param query:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def get_batch_scores(self, query, doc_ids):\n",
    "        \"\"\"\n",
    "        Calculate bm25 scores between query and subset of all docs\n",
    "        \"\"\"\n",
    "        assert all(di < len(self.doc_freqs) for di in doc_ids)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BM25Plus(BM25):\n",
    "    def __init__(self, corpus, tokenizer=None, k1=1.5, b=0.75, delta=1):\n",
    "        # Algorithm specific parameters\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.delta = delta\n",
    "        super().__init__(corpus, tokenizer)\n",
    "\n",
    "    def _calc_idf(self, nd):\n",
    "        \"\"\"\n",
    "        -\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def get_scores(self, query):\n",
    "        \"\"\"-\"\"\"\n",
    "\n",
    "    def get_batch_scores(self, query, doc_ids):\n",
    "        \"\"\"\n",
    "        Calculate bm25 scores between query and subset of all docs\n",
    "        \"\"\"\n",
    "        assert all(di < len(self.doc_freqs) for di in doc_ids)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bbed50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ed8288f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:17:07.828835Z",
     "start_time": "2021-11-08T19:17:07.825995Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Hello there good man!\",\n",
    "    \"It is quite windy in London\",\n",
    "    \"How is the weather today?\"\n",
    "]\n",
    "\n",
    "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "73dcd056",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:18:06.651590Z",
     "start_time": "2021-11-08T19:18:06.648691Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"windy London\"\n",
    "tokenized_query = query.split(\" \")\n",
    "\n",
    "doc_scores = bm25.get_scores(tokenized_query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e59c32f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:18:22.928821Z",
     "start_time": "2021-11-08T19:18:22.925221Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello there good man!']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.get_top_n(tokenized_query, corpus, n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f622b217",
   "metadata": {},
   "source": [
    "Возьмем BM25 из Implicit для рекомендаий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9283f500",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:37.733966Z",
     "start_time": "2021-11-08T19:20:37.729921Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from itertools import islice, cycle\n",
    "from more_itertools import pairwise\n",
    "import scipy.sparse as sp\n",
    "from itertools import islice, cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b236aaf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "083ef6e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:38.203182Z",
     "start_time": "2021-11-08T19:20:38.110389Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/interactions_preprocessed.pickle')\n",
    "df_users = pd.read_pickle('data/users_preprocessed.pickle')\n",
    "df_items = pd.read_pickle('data/items_preprocessed.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "47e71702",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:38.292831Z",
     "start_time": "2021-11-08T19:20:38.289153Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1532998, 5), (142888, 3), (59599, 5))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, df_users.shape, df_items.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d7c1ff2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:38.534174Z",
     "start_time": "2021-11-08T19:20:38.488706Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151600"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_inv_mapping = dict(enumerate(df['user_id'].unique()))\n",
    "users_mapping = {v: k for k, v in users_inv_mapping.items()}\n",
    "len(users_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "62e9c2fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:38.723923Z",
     "start_time": "2021-11-08T19:20:38.691925Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59599"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_inv_mapping = dict(enumerate(df['item_id'].unique()))\n",
    "items_mapping = {v: k for k, v in items_inv_mapping.items()}\n",
    "len(items_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5a79a273",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:38.828166Z",
     "start_time": "2021-11-08T19:20:38.766729Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59599, 'ворон-челобитчик')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_items['title'] = df_items['title'].str.lower()\n",
    "item_titles = pd.Series(df_items['title'].values, index=df_items['id']).to_dict()\n",
    "len(item_titles), item_titles[128115]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c07938b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:40.189022Z",
     "start_time": "2021-11-08T19:20:39.117397Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title\n",
       "# 20 восьмая                                                     [201623]\n",
       "# duo                                                             [72582]\n",
       "# me too. роман                                                  [171172]\n",
       "# партия                                                         [224512]\n",
       "#1917: человек из раньшего времени. библиотека «проекта 1917»    [230768]\n",
       "                                                                   ...   \n",
       "…чума на оба ваши дома!                                          [226481]\n",
       "№ 12, или история одного прекрасного юноши                        [20979]\n",
       "伦巴德人的故事                                                          [119226]\n",
       "地球への旅                                                            [148400]\n",
       "�baby blues�                                                      [98635]\n",
       "Name: id, Length: 57289, dtype: object"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_items = df_items.groupby('title')['id'].agg(list)\n",
    "title_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c81d3b32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:40.206585Z",
     "start_time": "2021-11-08T19:20:40.191307Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     55708\n",
       "2      1197\n",
       "3       245\n",
       "4        71\n",
       "5        38\n",
       "6        11\n",
       "7         8\n",
       "8         3\n",
       "9         2\n",
       "23        1\n",
       "18        1\n",
       "47        1\n",
       "13        1\n",
       "12        1\n",
       "11        1\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_count = title_items.map(len)\n",
    "title_count.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9d9b497f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:40.250323Z",
     "start_time": "2021-11-08T19:20:40.207999Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>progress</th>\n",
       "      <th>rating</th>\n",
       "      <th colspan=\"2\" halign=\"left\">start_date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44681</th>\n",
       "      <td>353</td>\n",
       "      <td>4.56</td>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>2019-12-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162716</th>\n",
       "      <td>59</td>\n",
       "      <td>4.80</td>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>2019-12-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        progress rating start_date           \n",
       "            size   mean        min        max\n",
       "item_id                                      \n",
       "44681        353   4.56 2018-01-24 2019-12-20\n",
       "162716        59   4.80 2018-01-25 2019-12-30"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rating'] = np.array(df['rating'].values, dtype=np.float32)\n",
    "\n",
    "df.loc[df['item_id'].isin([44681, 162716])].groupby('item_id').agg({\n",
    "    'progress': np.size,\n",
    "    'rating': ['mean'],\n",
    "    'start_date': ['min', 'max'],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e4c32f3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:40.260180Z",
     "start_time": "2021-11-08T19:20:40.252435Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2019-12-24 00:00:00'), Timestamp('2019-12-31 00:00:00'))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_date = df['start_date'].max().normalize()\n",
    "folds = 7\n",
    "start_date = last_date - pd.Timedelta(days=folds)\n",
    "start_date, last_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "81ebc1f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:40.274425Z",
     "start_time": "2021-11-08T19:20:40.261714Z"
    }
   },
   "outputs": [],
   "source": [
    "class TimeRangeSplit():\n",
    "    \"\"\"\n",
    "        https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.date_range.html\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 start_date, \n",
    "                 end_date=None, \n",
    "                 freq='D', \n",
    "                 periods=None, \n",
    "                 tz=None, \n",
    "                 normalize=False, \n",
    "                 closed=None, \n",
    "                 train_min_date=None,\n",
    "                 filter_cold_users=True, \n",
    "                 filter_cold_items=True, \n",
    "                 filter_already_seen=True):\n",
    "        \n",
    "        self.start_date = start_date\n",
    "        if end_date is None and periods is None:\n",
    "            raise ValueError(\"Either 'end_date' or 'periods' must be non-zero, not both at the same time.\")\n",
    "\n",
    "        self.end_date = end_date\n",
    "        self.freq = freq\n",
    "        self.periods = periods\n",
    "        self.tz = tz\n",
    "        self.normalize = normalize\n",
    "        self.closed = closed\n",
    "        self.train_min_date = pd.to_datetime(train_min_date, errors='raise')\n",
    "        self.filter_cold_users = filter_cold_users\n",
    "        self.filter_cold_items = filter_cold_items\n",
    "        self.filter_already_seen = filter_already_seen\n",
    "\n",
    "        self.date_range = pd.date_range(\n",
    "            start=start_date, \n",
    "            end=end_date, \n",
    "            freq=freq, \n",
    "            periods=periods, \n",
    "            tz=tz, \n",
    "            normalize=normalize, \n",
    "            closed=closed)\n",
    "\n",
    "        self.max_n_splits = max(0, len(self.date_range) - 1)\n",
    "        if self.max_n_splits == 0:\n",
    "            raise ValueError(\"Provided parametrs set an empty date range.\") \n",
    "\n",
    "    def split(self, \n",
    "              df, \n",
    "              user_column='user_id',\n",
    "              item_column='item_id',\n",
    "              datetime_column='date',\n",
    "              fold_stats=False):\n",
    "        df_datetime = df[datetime_column]\n",
    "        if self.train_min_date is not None:\n",
    "            train_min_mask = df_datetime >= self.train_min_date\n",
    "        else:\n",
    "            train_min_mask = df_datetime.notnull()\n",
    "\n",
    "        date_range = self.date_range[(self.date_range >= df_datetime.min()) & \n",
    "                                     (self.date_range <= df_datetime.max())]\n",
    "\n",
    "        for start, end in pairwise(date_range):\n",
    "            fold_info = {\n",
    "                'Start date': start,\n",
    "                'End date': end\n",
    "            }\n",
    "            train_mask = train_min_mask & (df_datetime < start)\n",
    "            train_idx = df.index[train_mask]\n",
    "            if fold_stats:\n",
    "                fold_info['Train'] = len(train_idx)\n",
    "\n",
    "            test_mask = (df_datetime >= start) & (df_datetime < end)\n",
    "            test_idx = df.index[test_mask]\n",
    "            \n",
    "            if self.filter_cold_users:\n",
    "                new = np.setdiff1d(\n",
    "                    df.loc[test_idx, user_column].unique(), \n",
    "                    df.loc[train_idx, user_column].unique())\n",
    "                new_idx = df.index[test_mask & df[user_column].isin(new)]\n",
    "                test_idx = np.setdiff1d(test_idx, new_idx)\n",
    "                test_mask = df.index.isin(test_idx)\n",
    "                if fold_stats:\n",
    "                    fold_info['New users'] = len(new)\n",
    "                    fold_info['New users interactions'] = len(new_idx)\n",
    "\n",
    "            if self.filter_cold_items:\n",
    "                new = np.setdiff1d(\n",
    "                    df.loc[test_idx, item_column].unique(), \n",
    "                    df.loc[train_idx, item_column].unique())\n",
    "                new_idx = df.index[test_mask & df[item_column].isin(new)]\n",
    "                test_idx = np.setdiff1d(test_idx, new_idx)\n",
    "                test_mask = df.index.isin(test_idx)\n",
    "                if fold_stats:\n",
    "                    fold_info['New items'] = len(new)\n",
    "                    fold_info['New items interactions'] = len(new_idx)\n",
    "\n",
    "            if self.filter_already_seen:\n",
    "                user_item = [user_column, item_column]\n",
    "                train_pairs = df.loc[train_idx, user_item].set_index(user_item).index\n",
    "                test_pairs = df.loc[test_idx, user_item].set_index(user_item).index\n",
    "                intersection = train_pairs.intersection(test_pairs)\n",
    "                test_idx = test_idx[~test_pairs.isin(intersection)]\n",
    "                # test_mask = rd.df.index.isin(test_idx)\n",
    "                if fold_stats:\n",
    "                    fold_info['Known interactions'] = len(intersection)\n",
    "\n",
    "            if fold_stats:\n",
    "                fold_info['Test'] = len(test_idx)\n",
    "\n",
    "            yield (train_idx, test_idx, fold_info)\n",
    "\n",
    "    def get_n_splits(self, df, datetime_column='date'):\n",
    "        df_datetime = df[datetime_column]\n",
    "        if self.train_min_date is not None:\n",
    "            df_datetime = df_datetime[df_datetime >= self.train_min_date]\n",
    "\n",
    "        date_range = self.date_range[(self.date_range >= df_datetime.min()) & \n",
    "                                     (self.date_range <= df_datetime.max())]\n",
    "\n",
    "        return max(0, len(date_range) - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5710d5b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:40.283763Z",
     "start_time": "2021-11-08T19:20:40.275525Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2019-12-24 00:00:00'), Timestamp('2019-12-31 00:00:00'))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_date = df['start_date'].max().normalize()\n",
    "folds = 7\n",
    "start_date = last_date - pd.Timedelta(days=folds)\n",
    "start_date, last_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7e04921f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:40.396858Z",
     "start_time": "2021-11-08T19:20:40.381832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = TimeRangeSplit(start_date=start_date, periods=folds+1)\n",
    "\n",
    "cv.max_n_splits, cv.get_n_splits(df, datetime_column='start_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "65dc9413",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:50.273205Z",
     "start_time": "2021-11-08T19:20:40.592465Z"
    }
   },
   "outputs": [],
   "source": [
    "folds_with_stats = list(cv.split(\n",
    "    df, \n",
    "    user_column='user_id',\n",
    "    item_column='item_id',\n",
    "    datetime_column='start_date',\n",
    "    fold_stats=True\n",
    "))\n",
    "\n",
    "folds_info_with_stats = pd.DataFrame([info for _, _, info in folds_with_stats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f6118116",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:50.278552Z",
     "start_time": "2021-11-08T19:20:50.274938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Timestamp('2019-12-24 00:00:00', freq='D'),\n",
       "  Timestamp('2019-12-25 00:00:00', freq='D')),\n",
       " (Timestamp('2019-12-25 00:00:00', freq='D'),\n",
       "  Timestamp('2019-12-26 00:00:00', freq='D')),\n",
       " (Timestamp('2019-12-26 00:00:00', freq='D'),\n",
       "  Timestamp('2019-12-27 00:00:00', freq='D')),\n",
       " (Timestamp('2019-12-27 00:00:00', freq='D'),\n",
       "  Timestamp('2019-12-28 00:00:00', freq='D')),\n",
       " (Timestamp('2019-12-28 00:00:00', freq='D'),\n",
       "  Timestamp('2019-12-29 00:00:00', freq='D')),\n",
       " (Timestamp('2019-12-29 00:00:00', freq='D'),\n",
       "  Timestamp('2019-12-30 00:00:00', freq='D')),\n",
       " (Timestamp('2019-12-30 00:00:00', freq='D'),\n",
       "  Timestamp('2019-12-31 00:00:00', freq='D'))]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_dates = [(info['Start date'], info['End date']) for _, _, info in folds_with_stats]\n",
    "fold_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3a127e6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:50.283182Z",
     "start_time": "2021-11-08T19:20:50.279547Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(df_true, df_pred, top_N):\n",
    "    result = {}\n",
    "    test_recs = df_true.set_index(['user_id', 'item_id']).join(df_pred.set_index(['user_id', 'item_id']))\n",
    "    test_recs = test_recs.sort_values(by=['user_id', 'rank'])\n",
    "\n",
    "    test_recs['users_item_count'] = test_recs.groupby(level='user_id')['rank'].transform(np.size)\n",
    "    test_recs['reciprocal_rank'] = (1 / test_recs['rank']).fillna(0)\n",
    "    test_recs['cumulative_rank'] = test_recs.groupby(level='user_id').cumcount() + 1\n",
    "    test_recs['cumulative_rank'] = test_recs['cumulative_rank'] / test_recs['rank']\n",
    "    \n",
    "    # код из прошлого заданния про метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cf31f3a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:50.382745Z",
     "start_time": "2021-11-08T19:20:50.284384Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1515946, 5), (2045, 5))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx, test_idx, info = folds_with_stats[0]\n",
    "\n",
    "train = df.loc[train_idx]\n",
    "test = df.loc[test_idx]\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "765d64c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:50.387178Z",
     "start_time": "2021-11-08T19:20:50.383819Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_coo_matrix(df, \n",
    "                   user_col='user_id', \n",
    "                   item_col='item_id', \n",
    "                   weight_col=None, \n",
    "                   users_mapping=users_mapping, \n",
    "                   items_mapping=items_mapping):\n",
    "    if weight_col is None:\n",
    "        weights = np.ones(len(df), dtype=np.float32)\n",
    "    else:\n",
    "        weights = df[weight_col].astype(np.float32)\n",
    "\n",
    "    interaction_matrix = sp.coo_matrix((\n",
    "        weights, \n",
    "        (\n",
    "            df[user_col].map(users_mapping.get), \n",
    "            df[item_col].map(items_mapping.get)\n",
    "        )\n",
    "    ))\n",
    "    return interaction_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3e63deae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:51.879501Z",
     "start_time": "2021-11-08T19:20:50.388266Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<151589x59599 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 1515946 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mat = get_coo_matrix(train).tocsr()\n",
    "train_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4edf1bbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:51.882777Z",
     "start_time": "2021-11-08T19:20:51.880624Z"
    }
   },
   "outputs": [],
   "source": [
    "from implicit.nearest_neighbours import CosineRecommender, TFIDFRecommender, BM25Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b3289dc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:53.835247Z",
     "start_time": "2021-11-08T19:20:51.883907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa8d445c4404aaba0177993c9da0d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59599 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cosine_model = CosineRecommender(K=10)\n",
    "cosine_model.fit(train_mat.T) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7d4d5271",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:55.945861Z",
     "start_time": "2021-11-08T19:20:53.836444Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a473a0b8e34c7b95e71439d80a7dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59599 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "md = BM25Recommender()\n",
    "md.fit(train_mat.T) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d64f75d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:55.950906Z",
     "start_time": "2021-11-08T19:20:55.947750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Рекомендации для пользователя 139942, номер строки - 10346\n"
     ]
    }
   ],
   "source": [
    "top_N = 10\n",
    "user_id = test['user_id'].iloc[0]\n",
    "row_id = users_mapping[user_id]\n",
    "print(f'Рекомендации для пользователя {user_id}, номер строки - {row_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "841f76a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:55.958346Z",
     "start_time": "2021-11-08T19:20:55.951972Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_id</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4341</td>\n",
       "      <td>0.297014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7353</td>\n",
       "      <td>0.220847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36593</td>\n",
       "      <td>0.215622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>0.188025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51215</td>\n",
       "      <td>0.145095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>49085</td>\n",
       "      <td>0.128586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>37852</td>\n",
       "      <td>0.102340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7873</td>\n",
       "      <td>0.101929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>46769</td>\n",
       "      <td>0.100504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>56270</td>\n",
       "      <td>0.100504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col_id  similarity\n",
       "0    4341    0.297014\n",
       "1    7353    0.220847\n",
       "2   36593    0.215622\n",
       "3    3802    0.188025\n",
       "4   51215    0.145095\n",
       "5   49085    0.128586\n",
       "6   37852    0.102340\n",
       "7    7873    0.101929\n",
       "8   46769    0.100504\n",
       "9   56270    0.100504"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recs = cosine_model.recommend(row_id, train_mat, N=top_N, filter_already_liked_items=True)\n",
    "recs = pd.DataFrame(recs, columns=['col_id', 'similarity'])\n",
    "recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5fee21e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:55.966577Z",
     "start_time": "2021-11-08T19:20:55.959294Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_id</th>\n",
       "      <th>similarity</th>\n",
       "      <th>item_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4341</td>\n",
       "      <td>0.297014</td>\n",
       "      <td>193358</td>\n",
       "      <td>#охотник на волков</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7353</td>\n",
       "      <td>0.220847</td>\n",
       "      <td>125586</td>\n",
       "      <td>меч предназначения</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36593</td>\n",
       "      <td>0.215622</td>\n",
       "      <td>203882</td>\n",
       "      <td>#имя для лис</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>0.188025</td>\n",
       "      <td>90986</td>\n",
       "      <td>кровь эльфов</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51215</td>\n",
       "      <td>0.145095</td>\n",
       "      <td>146180</td>\n",
       "      <td>крещение руси</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>49085</td>\n",
       "      <td>0.128586</td>\n",
       "      <td>264849</td>\n",
       "      <td>сплав закона</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>37852</td>\n",
       "      <td>0.102340</td>\n",
       "      <td>238155</td>\n",
       "      <td>владычица озера</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7873</td>\n",
       "      <td>0.101929</td>\n",
       "      <td>7094</td>\n",
       "      <td>крещение огнем</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>46769</td>\n",
       "      <td>0.100504</td>\n",
       "      <td>240226</td>\n",
       "      <td>отпущение без грехов</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>56270</td>\n",
       "      <td>0.100504</td>\n",
       "      <td>1728</td>\n",
       "      <td>«злой город»</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col_id  similarity  item_id                 title\n",
       "0    4341    0.297014   193358    #охотник на волков\n",
       "1    7353    0.220847   125586    меч предназначения\n",
       "2   36593    0.215622   203882          #имя для лис\n",
       "3    3802    0.188025    90986          кровь эльфов\n",
       "4   51215    0.145095   146180         крещение руси\n",
       "5   49085    0.128586   264849          сплав закона\n",
       "6   37852    0.102340   238155       владычица озера\n",
       "7    7873    0.101929     7094        крещение огнем\n",
       "8   46769    0.100504   240226  отпущение без грехов\n",
       "9   56270    0.100504     1728          «злой город»"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recs['item_id'] = recs['col_id'].map(items_inv_mapping.get)\n",
    "recs['title'] = recs['item_id'].map(item_titles.get)\n",
    "recs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454465ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:55.969824Z",
     "start_time": "2021-11-08T19:20:55.967505Z"
    }
   },
   "source": [
    "### Выберете наилучшую модель и параметры из Implcict с точки зрения максимизации map@10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e73553c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c962a03",
   "metadata": {},
   "source": [
    "# DSSM Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b2d4cd",
   "metadata": {},
   "source": [
    "Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1fb78964",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:55.972634Z",
     "start_time": "2021-11-08T19:20:55.970816Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "61373d29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:55.975318Z",
     "start_time": "2021-11-08T19:20:55.973562Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import typing as tp\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b1577257",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:55.981960Z",
     "start_time": "2021-11-08T19:20:55.976287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>progress</th>\n",
       "      <th>rating</th>\n",
       "      <th>start_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126706</td>\n",
       "      <td>14433</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127290</td>\n",
       "      <td>140952</td>\n",
       "      <td>58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66991</td>\n",
       "      <td>198453</td>\n",
       "      <td>89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46791</td>\n",
       "      <td>83486</td>\n",
       "      <td>23</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79313</td>\n",
       "      <td>188770</td>\n",
       "      <td>88</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  progress  rating start_date\n",
       "0   126706    14433        80     NaN 2018-01-01\n",
       "1   127290   140952        58     NaN 2018-01-01\n",
       "2    66991   198453        89     NaN 2018-01-01\n",
       "3    46791    83486        23     5.0 2018-01-01\n",
       "4    79313   188770        88     5.0 2018-01-01"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "015d9d59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:20:55.998336Z",
     "start_time": "2021-11-08T19:20:55.982994Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_n_splits(df, datetime_column='start_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "26f651d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:21:09.328946Z",
     "start_time": "2021-11-08T19:21:00.036036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.46 s, sys: 834 ms, total: 9.29 s\n",
      "Wall time: 9.29 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start date</th>\n",
       "      <th>End date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-12-24</td>\n",
       "      <td>2019-12-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-12-25</td>\n",
       "      <td>2019-12-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-12-26</td>\n",
       "      <td>2019-12-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>2019-12-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-12-28</td>\n",
       "      <td>2019-12-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>2019-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>2019-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Start date   End date\n",
       "0 2019-12-24 2019-12-25\n",
       "1 2019-12-25 2019-12-26\n",
       "2 2019-12-26 2019-12-27\n",
       "3 2019-12-27 2019-12-28\n",
       "4 2019-12-28 2019-12-29\n",
       "5 2019-12-29 2019-12-30\n",
       "6 2019-12-30 2019-12-31"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "folds = list(cv.split(df, datetime_column='start_date'))\n",
    "pd.DataFrame([stats for _, _, stats in folds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1af5966b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:21:09.440159Z",
     "start_time": "2021-11-08T19:21:09.330205Z"
    }
   },
   "outputs": [],
   "source": [
    "df['user_id'] = df['user_id'].astype('category')\n",
    "df['item_id'] = df['item_id'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e099197b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:23:15.846855Z",
     "start_time": "2021-11-08T19:23:15.841055Z"
    }
   },
   "outputs": [],
   "source": [
    "class DSSM(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        uf_dim: int,\n",
    "        uf1_dim: int,\n",
    "        uf2_dim: int,\n",
    "        if_dim: int,\n",
    "        if1_dim: int,\n",
    "        if2_dim: int,\n",
    "        final_dim: int,\n",
    "        dropout: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.user_seq = nn.Sequential(\n",
    "            nn.Linear(uf_dim, uf1_dim),\n",
    "            nn.BatchNorm1d(uf1_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(uf1_dim, uf2_dim),\n",
    "            nn.BatchNorm1d(uf2_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(uf2_dim, final_dim),\n",
    "            nn.BatchNorm1d(final_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.item_seq = nn.Sequential(\n",
    "            nn.Linear(if_dim, if1_dim),\n",
    "            nn.BatchNorm1d(if1_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(if1_dim, if2_dim),\n",
    "            nn.BatchNorm1d(if2_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(if2_dim, final_dim),\n",
    "            nn.BatchNorm1d(final_dim),\n",
    "            nn.ReLU(),\n",
    "        )        \n",
    "        self.cosine = nn.CosineSimilarity()\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        user_features,\n",
    "        item_features,\n",
    "    ):\n",
    "        user_vec = self.user_seq(user_features)\n",
    "        item_vec = self.item_seq(item_features)\n",
    "        sim = self.cosine(user_vec, item_vec)\n",
    "        return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4277f7cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:23:27.047208Z",
     "start_time": "2021-11-08T19:23:27.038456Z"
    }
   },
   "outputs": [],
   "source": [
    "def csr_to_tensor(csr: sparse.csr_matrix, flatten=False) -> torch.Tensor:\n",
    "    arr = csr.toarray()\n",
    "    if flatten:\n",
    "        arr = arr.reshape(-1)\n",
    "    return torch.from_numpy(arr)\n",
    "\n",
    "\n",
    "def make_id_csr(ids: pd.Series, n_cols: int) -> sparse.csr_matrix:\n",
    "    csr = sparse.csr_matrix(\n",
    "        (\n",
    "            np.ones(len(ids)),\n",
    "            (\n",
    "                np.arange(len(ids)),\n",
    "                ids,\n",
    "            )\n",
    "        ),\n",
    "        shape=(len(ids), n_cols),\n",
    "    )\n",
    "    return csr\n",
    "\n",
    "def binary_accuracy(true: torch.Tensor, pred: torch.Tensor) -> torch.Tensor:\n",
    "    predicted_labels = torch.round(torch.sigmoid(pred))\n",
    "    correct = (predicted_labels == true).float()\n",
    "    acc = correct.sum(axis=0) / len(correct)\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "def process_epoch(\n",
    "    net: nn.Module,\n",
    "    loader: DataLoader,  # type: ignore\n",
    "    optimizer: optim.Optimizer,\n",
    "    criterion: nn.modules.loss._Loss,\n",
    "    device: torch.device,\n",
    "    process: str, # \"train\", \"eval\"\n",
    ") -> tp.Tuple[float, float, np.ndarray, np.ndarray]:\n",
    "    epoch_loss = 0.\n",
    "    epoch_acc = 0\n",
    "    n_batches = len(loader)\n",
    "    all_true = []\n",
    "    all_predictions = []\n",
    "\n",
    "    if process == \"train\":\n",
    "        net.train()\n",
    "    elif process == \"eval\":\n",
    "        net.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(process == \"train\"):\n",
    "        for batch in tqdm(loader, total=n_batches):\n",
    "            u_batch, i_batch, y_batch = [b.to(device) for b in batch]\n",
    "            pred = net(u_batch, i_batch)\n",
    "            loss = criterion(pred, y_batch.type_as(pred))\n",
    "            acc = binary_accuracy(y_batch, pred)\n",
    "\n",
    "            if process == \"train\":\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.detach().cpu().numpy()\n",
    "            \n",
    "            all_true.append(y_batch.detach().cpu().numpy())\n",
    "            all_predictions.append(torch.sigmoid(pred).detach().cpu().numpy())\n",
    "\n",
    "    return (\n",
    "        epoch_loss / n_batches, \n",
    "        epoch_acc / n_batches, \n",
    "        np.concatenate(all_true, axis=0),\n",
    "        np.concatenate(all_predictions, axis=0),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2e24bfd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:23:33.860456Z",
     "start_time": "2021-11-08T19:23:33.847618Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d7ce8599",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:23:34.203265Z",
     "start_time": "2021-11-08T19:23:34.098741Z"
    }
   },
   "outputs": [],
   "source": [
    "train_idx, test_idx, _ = folds[0]\n",
    "\n",
    "df_train = df.loc[train_idx]\n",
    "#df_train['user_id'].cat.remove_unused_categories(inplace=True)\n",
    "#df_train['item_id'].cat.remove_unused_categories(inplace=True)\n",
    "df_test = df.loc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "903c2378",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:23:34.588910Z",
     "start_time": "2021-11-08T19:23:34.339880Z"
    }
   },
   "outputs": [],
   "source": [
    "unq_users = np.unique(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            df_train['user_id'].values,\n",
    "            df_test['item_id'].values,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "unq_items = np.unique(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            df_train['user_id'].values,\n",
    "            df_test['item_id'].values,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "feadcd97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:23:35.579719Z",
     "start_time": "2021-11-08T19:23:35.575597Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id             category\n",
       "item_id             category\n",
       "progress                int8\n",
       "rating               float32\n",
       "start_date    datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0ed1304b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:23:36.544445Z",
     "start_time": "2021-11-08T19:23:36.538139Z"
    }
   },
   "outputs": [],
   "source": [
    "class RecDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        interactions_df: pd.DataFrame,\n",
    "        unq_u_ids=None,\n",
    "        unq_i_ids=None,\n",
    "        neg_share=0.5,\n",
    "    ):\n",
    "        u_ids_pos = interactions_df['user_id'].cat.codes.values\n",
    "        i_ids_pos = interactions_df['item_id'].cat.codes.values\n",
    "        \n",
    "        if unq_u_ids is None:\n",
    "            unq_u_ids = np.unique(u_ids_pos)\n",
    "        if unq_i_ids is None:\n",
    "            unq_i_ids = np.unique(i_ids_pos)\n",
    "            \n",
    "        assert np.isin(u_ids_pos, unq_u_ids).all()\n",
    "        assert np.isin(i_ids_pos, unq_i_ids).all()\n",
    "        \n",
    "        n_negs = int(len(interactions_df) * neg_share / (1 - neg_share))\n",
    "        \n",
    "        # take random, do not check (can be positives here, but probability is very low)\n",
    "        u_ids_neg = np.random.choice(unq_u_ids, n_negs)\n",
    "        i_ids_neg = np.random.choice(unq_i_ids, n_negs)\n",
    "        \n",
    "        user_features = make_id_csr(np.concatenate((u_ids_pos, u_ids_neg)), n_cols=unq_u_ids.size)\n",
    "        item_features = make_id_csr(np.concatenate((i_ids_pos, i_ids_neg)), n_cols=unq_i_ids.size)\n",
    "        \n",
    "        self.user_features = user_features.astype(np.float32)\n",
    "        self.item_features = item_features.astype(np.float32)\n",
    "        self.y = np.concatenate((np.ones(len(interactions_df)), np.zeros(n_negs)))\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        u_f = csr_to_tensor(self.user_features[index], flatten=True)\n",
    "        i_f = csr_to_tensor(self.item_features[index], flatten=True)\n",
    "        y = self.y[index]\n",
    "        return u_f, i_f, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.user_features.shape[0]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b026920f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:23:38.546605Z",
     "start_time": "2021-11-08T19:23:37.380756Z"
    }
   },
   "outputs": [],
   "source": [
    "train_idx, test_idx, _ = folds[0]\n",
    "\n",
    "df_train = df.loc[train_idx]\n",
    "df_train['user_id'].cat.remove_unused_categories(inplace=True)\n",
    "df_train['item_id'].cat.remove_unused_categories(inplace=True)\n",
    "\n",
    "df_test = df.loc[test_idx]\n",
    "df_test['user_id'].cat.remove_unused_categories(inplace=True)\n",
    "df_test['item_id'].cat.remove_unused_categories(inplace=True)\n",
    "\n",
    "unq_users = np.unique(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            df_train['user_id'].cat.codes.values,\n",
    "            df_test['user_id'].cat.codes.values,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "unq_items = np.unique(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            df_train['item_id'].cat.codes.values,\n",
    "            df_test['item_id'].cat.codes.values,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "train_data = RecDataset(df_train, unq_u_ids=unq_users, unq_i_ids=unq_items)\n",
    "test_data = RecDataset(df_test, unq_u_ids=unq_users, unq_i_ids=unq_items)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=128,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=128,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "983d64f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:24:27.727592Z",
     "start_time": "2021-11-08T19:24:23.925770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DSSM(\n",
       "  (user_seq): Sequential(\n",
       "    (0): Linear(in_features=151589, out_features=32, bias=True)\n",
       "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Dropout(p=0.5, inplace=False)\n",
       "    (8): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (9): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "  )\n",
       "  (item_seq): Sequential(\n",
       "    (0): Linear(in_features=59599, out_features=32, bias=True)\n",
       "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Dropout(p=0.5, inplace=False)\n",
       "    (8): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (9): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "  )\n",
       "  (cosine): CosineSimilarity()\n",
       ")"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = DSSM(\n",
    "    uf_dim=unq_users.size,\n",
    "    uf1_dim=32, #256\n",
    "    uf2_dim=16, #128\n",
    "    if_dim=unq_items.size,\n",
    "    if1_dim=32, #256\n",
    "    if2_dim=16, #128\n",
    "    final_dim=8, # 64\n",
    "    dropout=0.5,\n",
    ")\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "63391563",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:24:27.731752Z",
     "start_time": "2021-11-08T19:24:27.729129Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9cad63d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:27:10.556457Z",
     "start_time": "2021-11-08T19:24:33.357497Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4930666ef17423695a2f4f4636f10bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23687 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda2/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda2/lib/python3.7/threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/home/irsafilo/course/shad/venv/lib/python3.7/site-packages/torch/utils/data/_utils/pin_memory.py\", line 28, in _pin_memory_loop\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/anaconda2/lib/python3.7/multiprocessing/queues.py\", line 113, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/data/home/irsafilo/course/shad/venv/lib/python3.7/site-packages/torch/multiprocessing/reductions.py\", line 289, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/opt/anaconda2/lib/python3.7/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/opt/anaconda2/lib/python3.7/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/opt/anaconda2/lib/python3.7/multiprocessing/connection.py\", line 492, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/opt/anaconda2/lib/python3.7/multiprocessing/connection.py\", line 619, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_118702/3835885113.py\u001b[0m in \u001b[0;36mprocess_epoch\u001b[0;34m(net, loader, optimizer, criterion, device, process)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mu_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/irsafilo/course/shad/venv/lib/python3.7/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/irsafilo/course/shad/venv/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/irsafilo/course/shad/venv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/irsafilo/course/shad/venv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/irsafilo/course/shad/venv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/irsafilo/course/shad/venv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda2/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda2/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "best_valid_loss = float('inf')\n",
    "best_model_state: tp.Dict[str, torch.Tensor] = {}\n",
    "for epoch in range(1):\n",
    "    print(f\"Epoch {epoch + 1} started\")\n",
    "\n",
    "    train_loss, train_acc, train_true, train_pred = process_epoch(net, train_loader, optimizer, criterion, device, \"train\")\n",
    "    valid_loss, valid_acc, valid_true, valid_pred = process_epoch(net, test_loader, optimizer, criterion, device, \"eval\")\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        best_model_state = net.state_dict()\n",
    "    \n",
    "#     train_aucs = roc_auc_many(train_true, train_pred)\n",
    "#     valid_aucs = roc_auc_many(valid_true, valid_pred)\n",
    "    \n",
    "    print(f\"Epoch: {epoch + 1:02}\")\n",
    "    print(\n",
    "        f\"\\n\\t Train Loss: {train_loss:.3f}\"\n",
    "        f\"\\n\\t Train Accuracy: {train_acc.round(3)}\"\n",
    "#         f\"\\n\\t Train AUC: {np.array(train_aucs).round(3)}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"\\n\\t Valid Loss: {valid_loss:.3f}\"\n",
    "        f\"\\n\\t Valid Accuracy: {valid_acc.round(3)}\"\n",
    "#         f\"\\n\\t Valid AUC: {np.array(valid_aucs).round(3)}\"\n",
    "    )\n",
    "\n",
    "net.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e36ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f91885e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:27:14.866680Z",
     "start_time": "2021-11-08T19:27:14.859987Z"
    }
   },
   "outputs": [],
   "source": [
    "class RecTripletDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        interactions_df: pd.DataFrame,\n",
    "        user_als_factors: np.ndarray,\n",
    "        item_als_factors: np.ndarray,\n",
    "        n_negs_per_pos: int = 1,\n",
    "    ):\n",
    "        u_ids_pos = interactions_df[Columns.User].cat.codes.values\n",
    "        i_ids_pos = interactions_df[Columns.Item].cat.codes.values\n",
    "\n",
    "        u_ids_all = np.repeat(u_ids_pos, n_negs_per_pos)\n",
    "        i_ids_pos_all = np.repeat(i_ids_pos, n_negs_per_pos)\n",
    "        \n",
    "        # take random, do not check (can be positives here, but probability is very low)\n",
    "        i_ids_neg_all = np.random.choice(np.arange(item_als_factors.shape[0]), len(interactions_df) * n_negs_per_pos)\n",
    "        \n",
    "        user_features = user_als_factors[u_ids_all]\n",
    "        pos_item_features = item_als_factors[i_ids_pos_all]\n",
    "        neg_item_features = item_als_factors[i_ids_neg_all]\n",
    "        \n",
    "        self.user_features = torch.tensor(user_features.astype(np.float32))\n",
    "        self.pos_item_features = torch.tensor(pos_item_features.astype(np.float32))\n",
    "        self.neg_item_features = torch.tensor(neg_item_features.astype(np.float32))\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        u_f = self.user_features[index]\n",
    "        p_i_f = self.pos_item_features[index]\n",
    "        n_i_f = self.neg_item_features[index]\n",
    "        return u_f, p_i_f, n_i_f\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.user_features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b6755d46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:27:15.495061Z",
     "start_time": "2021-11-08T19:27:15.487011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d67aaf4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:27:19.570932Z",
     "start_time": "2021-11-08T19:27:18.413539Z"
    }
   },
   "outputs": [],
   "source": [
    "train_idx, test_idx, _ = folds[0]\n",
    "\n",
    "df_train = df.loc[train_idx]\n",
    "df_train['user_id'].cat.remove_unused_categories(inplace=True)\n",
    "df_train['item_id'].cat.remove_unused_categories(inplace=True)\n",
    "\n",
    "df_test = df.loc[test_idx]\n",
    "df_test['user_id'].cat.remove_unused_categories(inplace=True)\n",
    "df_test['item_id'].cat.remove_unused_categories(inplace=True)\n",
    "\n",
    "unq_users = np.unique(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            df_train['user_id'].cat.codes.values,\n",
    "            df_test['user_id'].cat.codes.values,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "unq_items = np.unique(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            df_train['item_id'].cat.codes.values,\n",
    "            df_test['item_id'].cat.codes.values,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "train_data = RecTripletDataset(df_train, unq_u_ids=unq_users, unq_i_ids=unq_items)\n",
    "test_data = RecTripletDataset(df_test, unq_u_ids=unq_users, unq_i_ids=unq_items)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=128,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=128,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32ea250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfd1ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d1bb795",
   "metadata": {},
   "source": [
    "# Алгоритмы ранжирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17a1d52f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:10:01.249970Z",
     "start_time": "2021-11-10T18:10:00.934510Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b2e77ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:10:03.043856Z",
     "start_time": "2021-11-10T18:10:03.037978Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"query_id\":[i for i in range(100) for j in range(10)],\n",
    "    \"var1\":np.random.random(size=(1000,)),\n",
    "    \"var2\":np.random.random(size=(1000,)),\n",
    "    \"var3\":np.random.random(size=(1000,)),\n",
    "    \"relevance\":list(np.random.permutation([0,0,0,0,0, 0,0,0,1,1]))*100\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "460bec90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:10:03.253134Z",
     "start_time": "2021-11-10T18:10:03.239318Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>var1</th>\n",
       "      <th>var2</th>\n",
       "      <th>var3</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.561881</td>\n",
       "      <td>0.403810</td>\n",
       "      <td>0.257535</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.368569</td>\n",
       "      <td>0.109705</td>\n",
       "      <td>0.253796</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.483362</td>\n",
       "      <td>0.241358</td>\n",
       "      <td>0.904186</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.268705</td>\n",
       "      <td>0.203767</td>\n",
       "      <td>0.975273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.872451</td>\n",
       "      <td>0.251767</td>\n",
       "      <td>0.367190</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id      var1      var2      var3  relevance\n",
       "0         0  0.561881  0.403810  0.257535          0\n",
       "1         0  0.368569  0.109705  0.253796          1\n",
       "2         0  0.483362  0.241358  0.904186          0\n",
       "3         0  0.268705  0.203767  0.975273          1\n",
       "4         0  0.872451  0.251767  0.367190          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca8a1282",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:10:03.429607Z",
     "start_time": "2021-11-10T18:10:03.422978Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>var1</th>\n",
       "      <th>var2</th>\n",
       "      <th>var3</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>99</td>\n",
       "      <td>0.771522</td>\n",
       "      <td>0.481742</td>\n",
       "      <td>0.557153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>99</td>\n",
       "      <td>0.844032</td>\n",
       "      <td>0.230350</td>\n",
       "      <td>0.532514</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>99</td>\n",
       "      <td>0.449098</td>\n",
       "      <td>0.454860</td>\n",
       "      <td>0.593834</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>99</td>\n",
       "      <td>0.548287</td>\n",
       "      <td>0.482068</td>\n",
       "      <td>0.348547</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>99</td>\n",
       "      <td>0.971861</td>\n",
       "      <td>0.747146</td>\n",
       "      <td>0.252781</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     query_id      var1      var2      var3  relevance\n",
       "995        99  0.771522  0.481742  0.557153          0\n",
       "996        99  0.844032  0.230350  0.532514          0\n",
       "997        99  0.449098  0.454860  0.593834          0\n",
       "998        99  0.548287  0.482068  0.348547          0\n",
       "999        99  0.971861  0.747146  0.252781          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3029b982",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:10:03.720198Z",
     "start_time": "2021-11-10T18:10:03.713696Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = df[:800]  # first 80%\n",
    "validation_df = df[800:]  # remaining 20%\n",
    "\n",
    "qids_train = train_df.groupby(\"query_id\")[\"query_id\"].count().to_numpy()\n",
    "X_train = train_df.drop([\"query_id\", \"relevance\"], axis=1)\n",
    "y_train = train_df[\"relevance\"]\n",
    "\n",
    "qids_validation = validation_df.groupby(\"query_id\")[\"query_id\"].count().to_numpy()\n",
    "X_validation = validation_df.drop([\"query_id\", \"relevance\"], axis=1)\n",
    "y_validation = validation_df[\"relevance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eee6d4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:10:06.023791Z",
     "start_time": "2021-11-10T18:10:06.020040Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qids_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f65a8cf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:10:06.301695Z",
     "start_time": "2021-11-10T18:10:06.299011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qids_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "005edd6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:10:06.479854Z",
     "start_time": "2021-11-10T18:10:06.474183Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1</th>\n",
       "      <th>var2</th>\n",
       "      <th>var3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0.971973</td>\n",
       "      <td>0.732868</td>\n",
       "      <td>0.544135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>0.998749</td>\n",
       "      <td>0.662839</td>\n",
       "      <td>0.118983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>0.796351</td>\n",
       "      <td>0.530038</td>\n",
       "      <td>0.817161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>0.832002</td>\n",
       "      <td>0.181786</td>\n",
       "      <td>0.668164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>0.665987</td>\n",
       "      <td>0.708151</td>\n",
       "      <td>0.178262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         var1      var2      var3\n",
       "800  0.971973  0.732868  0.544135\n",
       "801  0.998749  0.662839  0.118983\n",
       "802  0.796351  0.530038  0.817161\n",
       "803  0.832002  0.181786  0.668164\n",
       "804  0.665987  0.708151  0.178262"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19f42f44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:10:06.651706Z",
     "start_time": "2021-11-10T18:10:06.648794Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800    0\n",
       "801    1\n",
       "802    0\n",
       "803    1\n",
       "804    0\n",
       "Name: relevance, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "687f1d06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:10:06.907156Z",
     "start_time": "2021-11-10T18:10:06.905373Z"
    }
   },
   "outputs": [],
   "source": [
    "model = lightgbm.LGBMRanker(\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9e8eafc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:29:10.886295Z",
     "start_time": "2021-11-08T19:29:10.784371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's ndcg@10: 0.592282\n",
      "[20]\tvalid_0's ndcg@10: 0.585728\n",
      "[30]\tvalid_0's ndcg@10: 0.556054\n",
      "[40]\tvalid_0's ndcg@10: 0.568618\n",
      "[50]\tvalid_0's ndcg@10: 0.561787\n",
      "[60]\tvalid_0's ndcg@10: 0.556834\n",
      "[70]\tvalid_0's ndcg@10: 0.556469\n",
      "[80]\tvalid_0's ndcg@10: 0.556599\n",
      "[90]\tvalid_0's ndcg@10: 0.56553\n",
      "[100]\tvalid_0's ndcg@10: 0.564829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRanker(metric='ndcg', objective='lambdarank')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    group=qids_train,\n",
    "    eval_set=[(X_validation, y_validation)],\n",
    "    eval_group=[qids_validation],\n",
    "    eval_at=10,\n",
    "    verbose=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87d245b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b80e391b",
   "metadata": {},
   "source": [
    "### Реализуйте lambdamart model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "451a4651",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T18:17:22.819911Z",
     "start_time": "2021-11-09T18:16:44.843116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=1. Current value: min_data_in_leaf=1\n",
      "[-9.10475824 -9.0548607  -9.08570388 -9.05570503 -9.01704942 -8.88069647\n",
      " -8.78728659 -8.59960181 -8.16982701 -7.72353072 -7.26744734 -6.62802583\n",
      " -5.80051888 -4.81027984 -3.73161119 -2.38679752 -0.76874673  1.4244456\n",
      "  4.43755817  8.70047671]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from lightgbm import LGBMRanker\n",
    "\n",
    "N = 20\n",
    "y = np.arange(N)\n",
    "X = np.random.normal(size=(N, 2))\n",
    "X[:, 1] = y[0:]\n",
    "\n",
    "model = LGBMRanker(min_data=1, min_data_in_bin=1)\n",
    "model.fit(X, y, group=[N])\n",
    "print(model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abe9e987",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:10:13.914501Z",
     "start_time": "2021-11-10T18:10:13.866159Z"
    }
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostRanker, Pool, MetricVisualizer\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3be74261",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:10:14.087475Z",
     "start_time": "2021-11-10T18:10:14.083612Z"
    }
   },
   "outputs": [],
   "source": [
    "from catboost.datasets import msrank_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ff72907",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:10:14.393699Z",
     "start_time": "2021-11-10T18:10:14.274673Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('msrank_train.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88e279c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:10:14.555457Z",
     "start_time": "2021-11-10T18:10:14.442506Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('msrank_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9885a0a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:10:15.447268Z",
     "start_time": "2021-11-10T18:10:15.433439Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>62</td>\n",
       "      <td>11089534</td>\n",
       "      <td>2</td>\n",
       "      <td>116</td>\n",
       "      <td>64034</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>54</td>\n",
       "      <td>11089534</td>\n",
       "      <td>2</td>\n",
       "      <td>124</td>\n",
       "      <td>64034</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>3344</td>\n",
       "      <td>14</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>56</td>\n",
       "      <td>11089534</td>\n",
       "      <td>13</td>\n",
       "      <td>123</td>\n",
       "      <td>63933</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>256</td>\n",
       "      <td>49697</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1  2  3  4  5  6    7    8         9  ...  128       129  130  131  \\\n",
       "0  2.0  1  3  3  0  0  3  1.0  1.0  0.000000  ...   62  11089534    2  116   \n",
       "1  2.0  1  3  0  3  0  3  1.0  0.0  1.000000  ...   54  11089534    2  124   \n",
       "2  0.0  1  3  0  2  0  3  1.0  0.0  0.666667  ...   45         3    1  124   \n",
       "3  2.0  1  3  0  3  0  3  1.0  0.0  1.000000  ...   56  11089534   13  123   \n",
       "4  1.0  1  3  0  3  0  3  1.0  0.0  1.000000  ...   64         5    7  256   \n",
       "\n",
       "     132  133  134  135  136  137  \n",
       "0  64034   13    3    0    0  0.0  \n",
       "1  64034    1    2    0    0  0.0  \n",
       "2   3344   14   67    0    0  0.0  \n",
       "3  63933    1    3    0    0  0.0  \n",
       "4  49697    1   13    0    0  0.0  \n",
       "\n",
       "[5 rows x 138 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e374b848",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:10:15.743636Z",
     "start_time": "2021-11-10T18:10:15.731189Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1291</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>131</td>\n",
       "      <td>2</td>\n",
       "      <td>24231</td>\n",
       "      <td>4897</td>\n",
       "      <td>95</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1291</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>52316</td>\n",
       "      <td>55891</td>\n",
       "      <td>79</td>\n",
       "      <td>124</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1291</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>11040</td>\n",
       "      <td>6</td>\n",
       "      <td>44336</td>\n",
       "      <td>26674</td>\n",
       "      <td>81</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1291</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "      <td>10394</td>\n",
       "      <td>4079</td>\n",
       "      <td>34</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1291</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5820</td>\n",
       "      <td>32540</td>\n",
       "      <td>35</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1  2  3  4  5  6    7    8    9  ...  128    129  130    131  \\\n",
       "9995  1.0  1291  2  0  1  0  2  1.0  0.0  0.5  ...   22    131    2  24231   \n",
       "9996  0.0  1291  2  0  1  1  2  1.0  0.0  0.5  ...   39      0    1  52316   \n",
       "9997  2.0  1291  2  0  2  0  2  1.0  0.0  1.0  ...   14  11040    6  44336   \n",
       "9998  2.0  1291  2  0  1  0  2  1.0  0.0  0.5  ...   31    105    0  10394   \n",
       "9999  1.0  1291  2  1  1  2  2  1.0  0.5  0.5  ...   47      8    3   5820   \n",
       "\n",
       "        132  133  134  135  136  137  \n",
       "9995   4897   95  228    0    0  0.0  \n",
       "9996  55891   79  124    0    0  0.0  \n",
       "9997  26674   81  153    0    0  0.0  \n",
       "9998   4079   34  155    0    0  0.0  \n",
       "9999  32540   35  153    0    0  0.0  \n",
       "\n",
       "[5 rows x 138 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cbeab1",
   "metadata": {},
   "source": [
    "### Ranking problem\n",
    "\n",
    "Введем обозначения:\n",
    "* $Q = \\{q_1, \\dots, q_n\\}$ список запросов\n",
    "* $D_q = \\{d_{q1}, \\dots, d_{qm}\\}$ -- набор документов, полученных для группы $q$\n",
    "* $L_q = \\{l_{q1}, \\dots, l_{qm}\\}$ -- метки релевантности для объектов из набор $D_q$\n",
    "\n",
    "Каждый обьект $d_{qi}$ представлен в векторном пространстве признаков, описывающих связи между группой и объектом.\n",
    "\n",
    "Таким образом, каждая группа связана с набором объектов. Например, группа - это запрос, а объект - это документ, если мы ранжируем документы по поисковому запросу.\n",
    "\n",
    "Цель состоит в том, чтобы изучить функцию ранжирования $ f = f (d_ {qi}) $, такую, чтобы ранжирование объектов $ d_ {qi} $ для всех групп из $ Q $ основывалось на их оценках $ x_ {qi} = f (d_ {qi}) $ максимально приближенных к идеальному рейтингу согласно $ l_ {qi} $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d11452",
   "metadata": {},
   "source": [
    "### Ranking quality metrics:\n",
    "* __Precision__\n",
    "    $$ \\mbox{P}=\\frac{|\\{\\mbox{relevant docs}\\}\\cap\\{\\mbox{retrieved docs}\\}|}{|\\{\\mbox{retrieved docs}\\}|} $$\n",
    "* __Recall__\n",
    "    $$ \\mbox{R}=\\frac{|\\{\\mbox{relevant docs}\\}\\cap\\{\\mbox{retrieved docs}\\}|}{|\\{\\mbox{relevant docs}\\}|} $$\n",
    "    \n",
    "    Обозначение $ @ k $ означает, что метрика рассчитывается для первых $ k $ документов из списка ранжирования.\n",
    "\n",
    "     Например, если 1,2,5,7,9 - это ранги релевантных документов (перечисления начинаются с номера 1 из десяти полученных, то $ P @ 5 $ будет $\\frac{3}{5}$.\n",
    "\n",
    "* __Mean average precision (MAP)__\n",
    "    $$\\frac{1}{|Q|}\\sum_{q \\in Q} \\frac{1}{|\\mbox{relevant docs in } D_q|} \\sum_{k} P@k(q) \\times rel(q, k) $$\n",
    "    \n",
    "    Где $rel(q, k)$ - метка релевантности документа на k-й позиции в нашем рейтинге $ D_q $. Этот показатель вычисляет среднюю точность для запроса, взвешенного с учетом релевантности документов, а затем вычисляет среднее значение между всеми запросами.\n",
    "    \n",
    "* __Discounted cumulative gain (DCG)__\n",
    "    $$\\sum_{k=1}^{mq} \\frac{2 ^ {l_{qk}}}{\\log_2(k+1)}$$\n",
    "    \n",
    "    Эта метрика учитывает поведение пользователя: внимание пользователя находится наверху, а затем нелинейно уменьшается до конца.\n",
    "    \n",
    "* __NDCG__ - нормализованный DCG = DCG $ ~ / ~ $ IDCG, где IDCG - максимально возможное значение DCG с заданным набором меток релевантности.\n",
    "\n",
    "* __AverageGain__ - представляет среднее значение значений метки для объектов с определенными значениями верхней метки.\n",
    "* __[PFound](https://tech.yandex.com/catboost/doc/dg/references/pfound-docpage/#pfound)__\n",
    "    \n",
    "Больше на wiki: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)\n",
    "\n",
    "Параметр $ @ k $ для каждой метрики можно указать через параметр метрики «top», например «NDCG: top = 10» будет означать NDCG @ 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9d86f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3aef59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897eca9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4572916",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:10:22.221152Z",
     "start_time": "2021-11-10T18:10:22.189031Z"
    }
   },
   "outputs": [],
   "source": [
    "#from catboost.datasets import msrank_10k\n",
    "#train_df, test_df = msrank_10k()\n",
    "\n",
    "X_train = train_df.drop(['0', '1'], axis=1).values\n",
    "y_train = train_df['0'].values\n",
    "queries_train = train_df['1'].values\n",
    "\n",
    "X_test = test_df.drop(['0', '1'], axis=1).values\n",
    "y_test = test_df['0'].values\n",
    "queries_test = test_df['1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e6e27b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3952656e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:10:22.962813Z",
     "start_time": "2021-11-10T18:10:22.960009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "num_documents = X_train.shape[0]\n",
    "print(num_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a5e72e",
   "metadata": {},
   "source": [
    "Число фичей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91782b83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:10:25.247501Z",
     "start_time": "2021-11-10T18:10:25.244276Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7e673b",
   "metadata": {},
   "source": [
    "__Relevance labels statistics__\n",
    "\n",
    "0 - неактуально, 4 - очень актуально. В таблице представлено количество документов для каждого значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f375f48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:10:25.905233Z",
     "start_time": "2021-11-10T18:10:25.900767Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(2.0, 1326), (0.0, 5481), (1.0, 3000), (3.0, 142), (4.0, 51)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(y_train).items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6df8758d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:10:26.223572Z",
     "start_time": "2021-11-10T18:10:26.221500Z"
    }
   },
   "outputs": [],
   "source": [
    "max_relevance = np.max(y_train)\n",
    "y_train /= max_relevance\n",
    "y_test /= max_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7145410",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:10:26.678213Z",
     "start_time": "2021-11-10T18:10:26.674763Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 3., 0., ..., 0., 0., 0.],\n",
       "       [3., 0., 3., ..., 0., 0., 0.],\n",
       "       [3., 0., 2., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [2., 0., 2., ..., 0., 0., 0.],\n",
       "       [2., 0., 1., ..., 0., 0., 0.],\n",
       "       [2., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "529e25fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:10:26.939032Z",
     "start_time": "2021-11-10T18:10:26.936239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_queries = np.unique(queries_train).shape[0]\n",
    "num_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e8abaf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:10:27.127732Z",
     "start_time": "2021-11-10T18:10:27.093315Z"
    }
   },
   "outputs": [],
   "source": [
    "train = Pool(\n",
    "    data=X_train,\n",
    "    label=y_train,\n",
    "    group_id=queries_train\n",
    ")\n",
    "\n",
    "test = Pool(\n",
    "    data=X_test,\n",
    "    label=y_test,\n",
    "    group_id=queries_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3faa82",
   "metadata": {},
   "source": [
    "### Можем создать pools из файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a0f3f65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T18:17:25.102508Z",
     "start_time": "2021-11-09T18:17:23.424495Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = './msrank'\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "train_file = os.path.join(data_dir, 'train.csv')\n",
    "test_file = os.path.join(data_dir, 'test.csv')\n",
    "\n",
    "train_df.to_csv(train_file, index=False, header=False)\n",
    "test_df.to_csv(test_file, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a898708",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T18:17:25.107135Z",
     "start_time": "2021-11-09T18:17:25.104061Z"
    }
   },
   "outputs": [],
   "source": [
    "description_file = os.path.join(data_dir, 'dataset.cd')\n",
    "with open(description_file, 'w') as f:\n",
    "    f.write('0\\tLabel\\n')\n",
    "    f.write('1\\tQueryId\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea2f025",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ce2029\">Attention:</span> Все обьекты должны быть сгруппированы по group_id\n",
    "\n",
    "Например, если набор данных состоит из пяти документов\n",
    "\\ [d1, d2, d3, d4, d5 \\] с соответствующими запросами \\ [q1, q2, q2, q1, q2 \\], тогда набор данных должен выглядеть так:\n",
    "\n",
    "$$\\begin{pmatrix}\n",
    "    d_1, q_1, f_1\\\\\n",
    "    d_4, q_1, f_4\\\\\n",
    "    d_2, q_2, f_2\\\\\n",
    "    d_3, q_2, f_3\\\\\n",
    "    d_5, q_2, f_5\\\\\n",
    "\\end{pmatrix} \\hspace{6px} \\texttt{or} \\hspace{6px}\n",
    "\\begin{pmatrix}\n",
    "    d_2, q_2, f_2\\\\\n",
    "    d_3, q_2, f_3\\\\\n",
    "    d_5, q_2, f_5\\\\\n",
    "    d_1, q_1, f_1\\\\\n",
    "    d_4, q_1, f_4\\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "где $f_i$ это вектор фичей для i-го документа."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21ae4d2",
   "metadata": {},
   "source": [
    "### Сведение проблемы к задаче машинного обучения\n",
    "\n",
    "Первая и самая простая идея - попытаться предсказать релевантность документа $ l_q $, минимизируя RMSE.\n",
    "\n",
    "$$\\frac{1}{N}\\sqrt{ \\sum_q \\sum_{d_{qk}} \\left(f(d_{qk}) - l_{qk} \\right)^2 }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9e930ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T18:25:10.077523Z",
     "start_time": "2021-11-09T18:25:10.074652Z"
    }
   },
   "outputs": [],
   "source": [
    "default_parameters = {\n",
    "    'iterations': 2000,\n",
    "    'custom_metric': ['NDCG', 'PFound', 'AverageGain:top=10'],\n",
    "    'verbose': False,\n",
    "    'random_seed': 0,\n",
    "}\n",
    "\n",
    "parameters = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6f9a1dcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T18:25:11.728738Z",
     "start_time": "2021-11-09T18:25:11.725193Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_model(loss_function, additional_params=None, train_pool=train, test_pool=test):\n",
    "    parameters = deepcopy(default_parameters)\n",
    "    parameters['loss_function'] = loss_function\n",
    "    parameters['train_dir'] = loss_function\n",
    "    \n",
    "    if additional_params is not None:\n",
    "        parameters.update(additional_params)\n",
    "        \n",
    "    model = CatBoostRanker(**parameters)\n",
    "    #model.fit(train_pool, eval_set=test_pool, plot=True)\n",
    "    model.fit(train_pool, eval_set=test_pool, plot=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c79f3a92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T18:25:36.930522Z",
     "start_time": "2021-11-09T18:25:12.488036Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/irsafilo/course/shad/venv/lib/python3.7/site-packages/catboost/core.py:5765: RuntimeWarning: Regression loss ('RMSE') ignores an important ranking parameter 'group_id'\n",
      "  warnings.warn(\"Regression loss ('{}') ignores an important ranking parameter 'group_id'\".format(loss_function), RuntimeWarning)\n",
      "Custom logger is already specified. Specify more than one logger at same time is not thread safe."
     ]
    }
   ],
   "source": [
    "model = fit_model('RMSE', {'custom_metric': ['PrecisionAt:top=10', 'RecallAt:top=10', 'MAP:top=10']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892289b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e9fe10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cadb26e4",
   "metadata": {},
   "source": [
    "### Параметр весов групп\n",
    "Предположим, мы знаем, что одни запросы для нас важнее других. <br/>\n",
    "Слово «важность» используется здесь с точки зрения точности или качества прогнозов CatBoostRanker для заданных запросов. <br/>\n",
    "Вы можете передать эту дополнительную информацию алгоритму с помощью параметра group_weights. <br/>\n",
    "Под капотом CatBoostRanker использует эти веса в функции потерь, просто умножая их на групповое слагаемое. <br/>\n",
    "Таким образом, чем больше вес, тем больше внимания будет уделяться запросу. <br/>\n",
    "Приведем пример процедуры обучения со случайными весами запросов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "64b2d425",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T18:26:52.727113Z",
     "start_time": "2021-11-09T18:26:24.841778Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRanker at 0x7f821837f898>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_weights(queries):\n",
    "    query_set = np.unique(queries)\n",
    "    query_weights = np.random.uniform(size=query_set.shape[0])\n",
    "    weights = np.zeros(shape=queries.shape)\n",
    "    \n",
    "    for i, query_id in enumerate(query_set):\n",
    "        weights[queries == query_id] = query_weights[i]\n",
    "    \n",
    "    return weights\n",
    "    \n",
    "\n",
    "train_with_weights = Pool(\n",
    "    data=X_train,\n",
    "    label=y_train,\n",
    "    group_weight=create_weights(queries_train),\n",
    "    group_id=queries_train\n",
    ")\n",
    "\n",
    "test_with_weights = Pool(\n",
    "    data=X_test,\n",
    "    label=y_test,\n",
    "    group_weight=create_weights(queries_test),\n",
    "    group_id=queries_test\n",
    ")\n",
    "\n",
    "fit_model(\n",
    "    'RMSE', \n",
    "    additional_params={'train_dir': 'RMSE_weigths'}, \n",
    "    train_pool=train_with_weights,\n",
    "    test_pool=test_with_weights\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d706826f",
   "metadata": {},
   "source": [
    "Поэксперементируйте с весами, задав их сокореллировано релевантности, сокореллировано частоте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae5a582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "483ab931",
   "metadata": {},
   "source": [
    "## Особый случай: прогноз топ-1\n",
    "\n",
    "Когда-нибудь вы можете столкнуться с проблемой $ - $, вам нужно будет предсказать один из наиболее релевантных объектов для данного запроса. <br/>\n",
    "Для этого в CatBoostRanker есть режим __QuerySoftMax__.\n",
    "\n",
    "Предположим, что у вашего набора данных бинарный таргет: 1 $ - $ означает лучший документ для запроса, 0 $ - $ другие. <br/>\n",
    "Мы максимально увеличим вероятность того, что станем лучшим документом для данного запроса. <br/>\n",
    "Набор данных MSRANK не содержит двоичных меток, но, например, для метода __QuerySoftMax__ мы конвертируем его в этот формат, <br/> выбирая лучший документ для каждого запроса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c3ed50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3d04cec1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T18:29:30.709871Z",
     "start_time": "2021-11-09T18:29:30.705882Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_best_documents(labels, queries):\n",
    "    query_set = np.unique(queries)\n",
    "    num_queries = query_set.shape[0]\n",
    "    by_query_arg_max = {query: -1 for query in query_set}\n",
    "    \n",
    "    for i, query in enumerate(queries):\n",
    "        best_idx = by_query_arg_max[query]\n",
    "        if best_idx == -1 or labels[best_idx] < labels[i]:\n",
    "            by_query_arg_max[query] = i\n",
    "    \n",
    "    binary_best_docs = np.zeros(shape=labels.shape)\n",
    "    for arg_max in by_query_arg_max.values():\n",
    "        binary_best_docs[arg_max] = 1.\n",
    "        \n",
    "    return binary_best_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "755210e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T18:34:26.588091Z",
     "start_time": "2021-11-09T18:29:32.201848Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRanker at 0x7f821837f1d0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_docs_train = get_best_documents(y_train, queries_train)\n",
    "best_docs_test = get_best_documents(y_test, queries_test)\n",
    "\n",
    "train_with_weights = Pool(\n",
    "    data=X_train,\n",
    "    label=best_docs_train,\n",
    "    group_id=queries_train,\n",
    "    group_weight=create_weights(queries_train)\n",
    ")\n",
    "\n",
    "test_with_weights = Pool(\n",
    "    data=X_test,\n",
    "    label=best_docs_test,\n",
    "    group_id=queries_test,\n",
    "    group_weight=create_weights(queries_test)\n",
    ")\n",
    "\n",
    "fit_model(\n",
    "    'QuerySoftMax',\n",
    "    additional_params={'custom_metric': 'AverageGain:top=1'},\n",
    "    train_pool=train_with_weights,\n",
    "    test_pool=test_with_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b375a663",
   "metadata": {},
   "source": [
    "### Уменьшение проблемы, шаг 2\n",
    "\n",
    "Теперь посмотрим на пример релевантности документов:\n",
    "\n",
    "$$ \n",
    "    \\begin{align}\n",
    "    labels(q_1) &= \\begin{bmatrix}\n",
    "           4 \\\\\n",
    "           3 \\\\\n",
    "           3 \\\\\n",
    "           1\n",
    "         \\end{bmatrix},\n",
    "    labels(q_2) &= \\begin{bmatrix}\n",
    "           2 \\\\\n",
    "           1 \\\\\n",
    "           1 \\\\\n",
    "           0\n",
    "         \\end{bmatrix}\n",
    "   \\end{align}\n",
    "$$\n",
    "\n",
    "Это означает, что с функцией потерь RMSE мы уделяем больше внимания q1, чем q2.\n",
    "\n",
    "Чтобы избежать этой проблемы, мы вводим в RMSE коэффициент $ c_q $, который зависит только от запроса (и если факт равен среднему значению разницы между предсказанием и меткой).\n",
    "\n",
    "$$\\frac{1}{N}\\sqrt{ \\sum_q \\sum_{d_{qk}} \\left(f(d_{qk}) - l_{qk} - \\color{red}{c_{q}} \\right)^2 }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7b819f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97b7586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c1403974",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T18:34:53.414461Z",
     "start_time": "2021-11-09T18:34:26.590344Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRanker at 0x7f821837f5f8>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_model('QueryRMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bf4d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6970909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e0b802d",
   "metadata": {},
   "source": [
    "### Уменьшение проблемы, шаг 3\n",
    "\n",
    "Поскольку целью ранжирования является прогнозирование списка документов (который может быть сгенерирован на основе заданных релевантностей документов), функция потерь RMSE не учитывает отношения между документами: первый лучше, чем второй, второй лучше, чем третий и пятый и т. Д. .\n",
    "\n",
    "Мы можем легко внести эту информацию в функцию потерь, сведя проблему не к регрессии, а к классификации для двух документов. $ (D_i, d_j) $ - делает $ i $ ый лучше, чем $ j $ ый, или нет.\n",
    "\n",
    "Таким образом, мы минимизируем отрицательную логарифмическую вероятность:\n",
    "\n",
    "$$ - \\sum_{i, j \\in Pairs} \\log \\left (\\frac {1} {1 + \\exp {- (f (d_i) - f (d_j))}} \\right) $$\n",
    "\n",
    "Методы, основанные на парных сравнениях, называются __pairwise__ в CatBoostRanker, эта цель называется __PairLogit__.\n",
    "\n",
    "Нет необходимости изменять набор данных, которые CatBoost генерирует для нас. Количество генерируемых пар, управляемых параметром max_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5f0ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model('PairLogit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c2d8ec",
   "metadata": {},
   "source": [
    "Также мы можем указать пары напрямую. Это можно сделать двумя способами:\n",
    "\n",
    "1. Двумерная матрица с shape = (num_pairs, 2) $ \\rightarrow $ (Win_id, loser_id): list, numpy.array, pandas.DataFrame.\n",
    "2. Второй путь к входному файлу, содержащему описания пар:\n",
    "     * Формат строки: $ \\texttt {[индекс победителя, индекс проигравшего, вес пары]} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "654022a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T18:39:09.882356Z",
     "start_time": "2021-11-09T18:39:09.812087Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_groups(file_name):\n",
    "    groups = {}\n",
    "    group_ids = []\n",
    "\n",
    "    with open(file_name) as f:\n",
    "        for doc_id, line in enumerate(f):\n",
    "            line = line.split(',')[:2]\n",
    "            \n",
    "            label, query_id = float(line[0]), int(line[1])\n",
    "            if query_id not in groups:\n",
    "                groups[query_id] = []\n",
    "            groups[query_id].append((doc_id, label))\n",
    "\n",
    "            group_ids.append(query_id)\n",
    "\n",
    "    return groups, group_ids\n",
    "            \n",
    "train_groups, train_group_ids = read_groups(train_file)\n",
    "assert num_queries == len(train_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b909a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a8ec6a8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T18:39:21.092754Z",
     "start_time": "2021-11-09T18:39:20.344276Z"
    }
   },
   "outputs": [],
   "source": [
    "pairs = []\n",
    "\n",
    "for group in train_groups.values():\n",
    "    for i in range(len(group)):\n",
    "        for j in range(i, len(group)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            doc_i, relevance_i = group[i]\n",
    "            doc_j, relevance_j = group[j]\n",
    "            if relevance_i < relevance_j:\n",
    "                pairs.append((doc_j, doc_i))\n",
    "            else:\n",
    "                pairs.append((doc_i, doc_j))\n",
    "                \n",
    "pairs_file = os.path.join(data_dir, 'pairs.csv')\n",
    "\n",
    "with open(pairs_file, 'w') as f:\n",
    "    for pair in pairs:\n",
    "        f.write(str(pair[0]) + '\\t' + str(pair[1]) + '\\t1\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1b4b0cc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T18:39:23.141694Z",
     "start_time": "2021-11-09T18:39:21.963758Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Custom logger is already specified. Specify more than one logger at same time is not thread safe."
     ]
    }
   ],
   "source": [
    "pool1 = Pool(data=X_train, label=y_train, group_id=train_group_ids, pairs=pairs)\n",
    "pool2 = Pool(data=train_file, column_description=description_file, pairs=pairs_file, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2209b1",
   "metadata": {},
   "source": [
    "### шаг 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33117446",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model('PairLogitPairwise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1c48d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a028bce1",
   "metadata": {},
   "source": [
    "### Уменьшение проблемы, шаг 4\n",
    "\n",
    "Предыдущая функция потерь напрямую минимизировала количество пар $ (d_i, d_j) $, где $ l_i> l_j $, но $ f (d_i) <f (d_j) $, просто указала количество неправильно размещенных документов.\n",
    "\n",
    "Поскольку внимание пользователя уделяется первым документам и низким - последним, неправильное переключение первых двух документов и двух последних имеет разную стоимость.\n",
    "\n",
    "На шагах 3 и 3.1 пользователь может установить вес пары.\n",
    "\n",
    "Метод __YetiRank__ учитывает этот эффект и генерирует веса для пар в соответствии с их положением ([paper] (https://cache-mskstoredata08.cdn.yandex.net/download.yandex.ru/company/to_rank_challenge_with_yetirank.pdf)).\n",
    "\n",
    "$$ - \\sum_{i,j \\in Pairs} \\color{red}{w_{ij}} \\log \\left( \\frac{1}{1 + \\exp{-(f(d_i) - f(d_j))}} \\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d842b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T18:42:31.193185Z",
     "start_time": "2021-11-09T18:42:31.193174Z"
    }
   },
   "outputs": [],
   "source": [
    "fit_model('YetiRank')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6793e727",
   "metadata": {},
   "source": [
    "### Шаг 4.1\n",
    "\n",
    "Как и в шаге 3.1, __YetiRankPairwise__ медленнее, чем __YetiRank__, но дает более точные результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de37a4a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T18:42:31.194382Z",
     "start_time": "2021-11-09T18:42:31.194373Z"
    }
   },
   "outputs": [],
   "source": [
    "fit_model('YetiRankPairwise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0700e7c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T18:43:33.436862Z",
     "start_time": "2021-11-09T18:42:58.235355Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282dcd3d49554c9286fb6f31fab90f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_84427/189348924.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwidget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMetricVisualizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RMSE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'QueryRMSE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PairLogit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PairLogitPairwise'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'YetiRank'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'YetiRankPairwise'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwidget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/home/irsafilo/course/shad/venv/lib/python3.7/site-packages/catboost/widget/ipythonwidget.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_need_to_stop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda2/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda2/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "widget = MetricVisualizer(['RMSE', 'QueryRMSE', 'PairLogit', 'PairLogitPairwise', 'YetiRank', 'YetiRankPairwise'])\n",
    "widget.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f3d0e53f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T18:43:37.871311Z",
     "start_time": "2021-11-09T18:43:37.709855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7644b7dfc5147d680667128c88f0f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widget = MetricVisualizer(['RMSE', 'QueryRMSE'])\n",
    "widget.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223bf506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bc654b9",
   "metadata": {},
   "source": [
    "### Простая классификация\n",
    "\n",
    "Очень быстро $ \\rightarrow $ очень медленно; Простой метод $ \\rightarrow $ сложный метод; Низкое качество $ \\rightarrow $ высокое качество.\n",
    "\n",
    "1. RMSE\n",
    "2. QueryRMSE\n",
    "3. PairLogit\n",
    "4. PairLogitPairwise\n",
    "5. YetiRank\n",
    "6. YetiRankPairwise\n",
    "\n",
    "Помимо нашей классификации, качество конкретного метода может зависеть от вашего набора данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63944a41",
   "metadata": {},
   "source": [
    "Посмотрите на метрику NDCG метода YetiRank $ - $ она недостаточно приспособлена."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a72ae14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T18:42:31.196299Z",
     "start_time": "2021-11-09T18:42:31.196290Z"
    }
   },
   "outputs": [],
   "source": [
    "fit_model('YetiRank', {'train_dir': 'YetiRank-lr-0.3', 'learning_rate': 0.3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b86f11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T18:42:31.197465Z",
     "start_time": "2021-11-09T18:42:31.197456Z"
    }
   },
   "outputs": [],
   "source": [
    "widget = MetricVisualizer(['YetiRank', 'YetiRank-lr-0.3'])\n",
    "widget.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0498571a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f62a52c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T17:19:22.190932Z",
     "start_time": "2021-10-31T17:19:22.185240Z"
    }
   },
   "source": [
    "## Регрессия против классификации против LTR\n",
    "Все они обучаются c учителем. Но целевые переменные различаются. Алгоритм обучения будет отличаться тем, как мы математически формулируем цель обучения.\n",
    "\n",
    "**Регрессия**\n",
    "Мы пытаемся изучить функцию f (x) с учетом признака x, чтобы предсказать действительное значение y∈ℝ.\n",
    "\n",
    "Пример: я хочу спрогнозировать средний баланс CASA в следующие 7 дней для учетной записи.\n",
    "\n",
    "**Классификация**\n",
    "Мы пытаемся изучить функцию f (x) с учетом признака x, чтобы предсказать набор дискретных целочисленных меток y∈ {1,2, ..., N} с N -классами.\n",
    "\n",
    "**Пример**: я хочу предсказать, будет ли продлен данный FD (фиксированный депозит) по истечении текущего срока.\n",
    "\n",
    "**Обучение ранжированию**\n",
    "Мы пытаемся изучить функцию f (q, D), учитывая запрос q и соответствующий список элементов D, чтобы предсказать порядок (ранжирование) всех элементов в списке."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8583d3",
   "metadata": {},
   "source": [
    "## Классическая проблема в LTR\n",
    "\n",
    "**web-search ranking**\n",
    "\n",
    "Учитывая поисковый запрос, ранжируйте релевантность результирующих совпадающих URL-адресов документов, чтобы в первую очередь пользователю был представлен более релевантный документ.\n",
    "\n",
    "Более формально мы изображаем вышеуказанную проблему как следующую задачу:\n",
    "\n",
    "Учитывая запрос q и результирующие n документов D = d1, d2, ..., dn, мы хотели бы изучить функцию f, такую, чтобы f (q, D) предсказывала релевантность любого данного документа, связанного с запрос. В идеале функция f (q, D) должна возвращать упорядоченный список документов D , ранжированных от наиболее до наименее релевантных для данного запроса q.\n",
    "\n",
    "Популярные наборы данных веб-поиска для крупномасштабного теста LTR:\n",
    "\n",
    "Наборы данных Microsoft LTR\n",
    "\n",
    "Наборы данных Yahoo LTR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caa9619",
   "metadata": {},
   "source": [
    "## Неклассические проблемы в LTR\n",
    "LTR - это общий подход к решению задачи ранжирования. Вот еще несколько примеров, помимо рейтинга в веб-поиске. Обратите внимание, что не все из них, на первый взгляд, представляют собой задачу ранжирования.\n",
    "\n",
    "**Рекомендательная система** (Вычисление персональный рейтинг предпочтений продуктов)\n",
    "\n",
    "**Выбор портфеля акций** (Вычисление доходности акций)\n",
    "\n",
    "**Автоответчик на сообщение** (Определение рейтинга лучших кандидатов в рекомендациях по электронной почте / ответу на сообщение)\n",
    "\n",
    "**Изображение в текст** (Решение контекстной функции лучшего кандидата) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa014e8",
   "metadata": {},
   "source": [
    "## Типы  LTR алгоритмов:\n",
    "- Pointwise\n",
    "- Pairwise\n",
    "- Listwise\n",
    "\n",
    "Они отличаются тем, как мы формулируем **функцию потерь** в основной задаче машинного обучения.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10454fce",
   "metadata": {},
   "source": [
    "**Задача ранжирования**\n",
    "\n",
    "Учитывая запрос q и результирующий документ n D = d1, d2, ..., dn, мы хотели бы изучить функцию f, такую, чтобы f (q, D) предсказывала релевантность любого данного документа, связанного с запрос."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d075870f",
   "metadata": {},
   "source": [
    "## pairwise LTR\n",
    "При попарном подходе мы все еще пытаемся изучить функцию точечной оценки f (q, di), однако наши обучающие примеры теперь состоят из пар документов в одном запросе:\n",
    "\n",
    "x1: q1, (d1, d2)\n",
    "х2: q2, (d3, d4)\n",
    "x3: q2, (d3, d5)\n",
    "x4: q2, (d4, d5)\n",
    "\n",
    "При такой настройке можно получить новый набор попарных БИНАРНЫХ меток, просто сравнив индивидуальную оценку релевантности в каждой паре. Например, для первого запроса q1, если y1 = 0 (совершенно неактуально) для d1 и y2 = 3 (очень актуально) для d2, то у нас есть новая метка y1 <y2 для пары документов (d1, d2). **Теперь проблема превратилась в задачу обучения бинарной классификации.**\n",
    "\n",
    "Для того чтобы попарно изучить поточечную функцию f (q, di), мы моделируем разницу в баллах вероятностно:\n",
    " \n",
    "$Pr(i \\succ j) \\equiv \\frac{1}{1 + exp^{-(s_i - s_j)}}$\n",
    "Проще говоря, если документ i соответствует лучше, чем документ j (который мы обозначаем как i≻j), то вероятность того, что функция оценки получит оценку f (q, di) = si выше, чем f (q, dj) = sj должен быть близок к 1. Другими словами, модель пытается научиться, учитывая запрос, как оценить пару документов, чтобы более релевантный документ получил более высокую оценку.\n",
    "\n",
    "Плюсы:\n",
    "Модель учится ранжировать напрямую, хотя и только попарно, но теоретически она может приблизиться к производительности общей задачи ранжирования при наличии N документов в согласованном списке.\n",
    "Нам не нужны явные точечные метки. Требуются только парные предпочтения. Это преимущество, потому что иногда мы можем сделать вывод о парном предпочтении только на основе собранных данных о поведении пользователей.\n",
    "Минусы:\n",
    "Сама функция скоринга по-прежнему является точечной, а это означает, что относительная информация в пространстве функций среди разных документов по одному и тому же запросу все еще не используется полностью."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fb1f90",
   "metadata": {},
   "source": [
    "**Задача ранжирования**\n",
    "\n",
    "Учитывая запрос q и результирующий документ n D = d1, d2, ..., dn, мы хотели бы изучить функцию f, такую, чтобы f (q, D) предсказывала релевантность любого данного документа, связанного с запрос.\n",
    "\n",
    "## Listwise LTR\n",
    "ListNet - это первый предложенный списочный подход. Здесь мы объясняем, как он подходит к задаче ранжирования.\n",
    "\n",
    "ListNet основан на концепции вероятности перестановки с учетом ранжированного списка. Снова мы предполагаем, что есть функция точечной оценки f (q, di), используемая для оценки и, следовательно, ранжирования данного списка элементов. Но вместо моделирования вероятности попарного сравнения с использованием разницы оценок теперь мы хотели бы смоделировать вероятность всех результатов ранжирования.\n",
    "\n",
    "x1: q1, (d1, d2)\n",
    "\n",
    "x2: q2, (d3, d4, d5)\n",
    "\n",
    "**вероятность перестановки**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c44e65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T17:40:19.383323Z",
     "start_time": "2021-11-01T17:40:19.376769Z"
    }
   },
   "source": [
    "Обозначим π как конкретную перестановку данного списка длины n, ϕ (si) = f (q, di) как любую возрастающую функцию оценки si для данного запроса q и документа i. Вероятность наличия перестановки π может быть записана как\n",
    "\n",
    "$Pr(\\pi) = \\prod_{i=1}^n \\frac{\\phi(s_i)}{\\sum_{k=i}^n\\phi(s_k)}$\n",
    "\n",
    "Для иллюстрации, учитывая список из 3 элементов, вероятность возврата перестановки s1, s2, s3 рассчитывается как: \n",
    "\n",
    "$Pr(\\pi = \\{s_1, s_2, s_3\\}) = \\frac{\\phi(s_1)}{\\phi(s_1) + \\phi(s_2) + \\phi(s_3)} \\cdot \\frac{\\phi(s_2)}{\\phi(s_2) + \\phi(s_3)} \\cdot \\frac{\\phi(s_3)}{\\phi(s_3)}$\n",
    "\n",
    "Из-за вычислительной сложности ListNet упрощает задачу, рассматривая только первую вероятность данного элемента. Вероятность первого попадания объекта i равна сумме вероятностей перестановок, в которых объект i занимает первое место. В самом деле, вероятность первого попадания объекта i может быть записана как:\n",
    "\n",
    "$Pr(i) = \\frac{\\phi(s_i)}{\\sum_{k=1}^n \\phi(s_k)}$\n",
    "\n",
    "Теперь, учитывая любые два списка ранжирования, представленные вероятностями первого числа, мы можем измерить их разницу с помощью перекрестной энтропии. Затем мы можем построить алгоритм машинного обучения, который минимизирует перекрестную энтропию.\n",
    "\n",
    "Для выбора функции ϕ (⋅) это может быть так же просто, как просто экспоненциальная функция. В самом деле, когда ϕ (⋅) экспоненциально, а длина списка равна двум, решение сведется к попарной модели, которую мы только что описали в предыдущем разделе.\n",
    "\n",
    "- Плюсы:\n",
    "\n",
    "     - Теоретически обоснованное решение подойти к задаче ранжирования.\n",
    "\n",
    "- Минусы:\n",
    "\n",
    "     - Дорогостоящие вычисления в теоретической форме, поэтому на практике используются несколько приближений. (Например, использование первой вероятности.)\n",
    "     - Функция подсчета очков по-прежнему точечная, что может быть неоптимальным."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e812543",
   "metadata": {},
   "source": [
    "## Эволюция LTR Model\n",
    "Как мы оцениваем результат прогнозирования рейтинга?\n",
    "\n",
    "Было предложено несколько показателей, которые обычно используются при оценке модели ранжирования:\n",
    "\n",
    "- Бинарная релевантность\n",
    "     - Средняя средняя точность (MAP)\n",
    "     - Средний взаимный ранг (MRR)\n",
    "- Оценка релевантности\n",
    "     - Нормализованная дисконтированная совокупная прибыль (NDCG)\n",
    "     - Ожидаемый взаимный ранг (ERR)\n",
    "    \n",
    "Как правило, бинарные меры учитывают только релевантные и сравнительные данные. нерелевантными, в то время как при градуированных показателях также будет учитываться ранжирование среди релевантных элементов. В этом случае при оценке рейтингового списка имеет значение степень релевантности."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb4eea7",
   "metadata": {},
   "source": [
    "## Labeling Issues\n",
    "Вообще говоря, есть два подхода к маркировке набора данных ранжирования:\n",
    "\n",
    "- Человеческое суждение\n",
    "- Вывод из log поведения пользователя\n",
    "\n",
    "Для первого подхода требуется огромная рабочая сила, чтобы обозначить релевантность каждого элемента для данного запроса. В реальном мире многие наборы данных не могут быть помечены таким образом, поэтому мы полагаемся на второй подход, который косвенно определяет предпочтения пользователя среди различных элементов.\n",
    "\n",
    "Обычно парное предпочтение может быть получено из взаимодействия пользователя с результатом запроса. Например, используйте данные о кликах, чтобы сделать вывод о релевантности веб-поиска. Вот почему попарный подход в LTR может привлечь гораздо больше внимания, чем точечный метод: из-за доступности данных.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b46e7f",
   "metadata": {},
   "source": [
    "## From Pairwise to Listwise, and More\n",
    "Недавно исследователи из Google обобщили структуру LambdaMART, чтобы предоставить теоретические основы модели ранжирования всех трех типов функции потерь (точечной, попарной, списочной) и прямой оптимизации всех популярных показателей ранжирования (NDCG, MAP, ... ). Фреймворк называется LambdaLoss (2018).\n",
    "\n",
    "Готовая к производству реализация такого фреймворка также находится в открытом доступе в виде модуля ранжирования в популярной библиотеке TensorFlow. Также предлагается функция групповой оценки, которая может быть реализована в библиотеке.\n",
    "\n",
    "Причина, по которой мы решили специально разработать вышеупомянутые модели, заключается в том, что они являются самой основой литературы LTR, цитируемой более тысячи раз.\n",
    "\n",
    "И причина, по которой выбираются библиотеки, в основном та же: это самые современные популярные фреймворки с открытым исходным кодом в этой области.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6510e1b",
   "metadata": {},
   "source": [
    "## RankNet\n",
    "Помните, что мы моделируем разницу оценок между данной парой (i, j) как вероятность, основанную на сигмовидной функции:\n",
    "\n",
    "$Pr(i \\succ j) = P_{ij} \\equiv \\frac{1}{1 + exp^{-(s_i - s_j)}}$\n",
    "где\n",
    "\n",
    "si=f(q,di) \n",
    "\n",
    "это точечный результат, полученный нашим алгоритмом f (q, d), который в RankNet сформулирован как двухуровневая нейронная сеть, параметризованная набором wk. (Или даже подумайте проще, пусть f (q, di) = wxi как линейный алгоритм.)\n",
    "\n",
    "Учитывая распределение вероятностей p, энтропия определяется как: p⋅log21p. Теперь пусть yij∈ {0,1} будет фактической меткой данной пары (i, j). Функция потерь в приведенной выше настройке будет перекрестной энтропией:\n",
    "\n",
    "$loss = -\\sum_{i \\neq j}{y_{ij}log_2P_{ij} + (1-y_{ij})log_2(1-P_{ij})}$\n",
    "\n",
    "Кросс-энтропия измеряет, насколько близки два распределения вероятностей друг к другу. Естественно, это хорошая целевая функция для модели машинного обучения, моделирующей вероятность оптимизации. Используя технологию обратного распространения, мы можем численно найти веса модели в f (q, d), которые минимизируют потерю кросс-энтропии.\n",
    "\n",
    "Обратите внимание, что приведенная выше потеря носит очень общий характер: это просто ожидаемая логарифмическая потеря или сумма кросс-энтропии из каждого обучающего примера, используемая для измерения того, насколько хорошо распределение модели приближается к эмпирическому распределению данных обучения (которое, в свою очередь, служит приближением к неизвестному истинному распределению, генерирующему обучающие данные). Мы можем легко поменять местами нейронную сеть с другими учащимися, что приведет к множеству различных попарных моделей LTR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd4b33d",
   "metadata": {},
   "source": [
    "## LambdaNet\n",
    "От RankNet до LambdaNet были достигнуты два важных усовершенствования.\n",
    "\n",
    "1.Ускорение обучения за счет факторизации расчета градиента\n",
    "\n",
    "2) Оптимизация по метрике ранжирования"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e472753d",
   "metadata": {},
   "source": [
    "## Gradient Factorization\n",
    "Во-первых, LambdaNet - это **математически улучшенная версия RankNet**. Улучшение основано на факторизации вычисления градиента потери кросс-энтропии в контексте попарного обновления.\n",
    "\n",
    "Учитывая потерю точечной кросс-энтропии как L:\n",
    "\n",
    "$L = y_{ij}log_2P_{ij} + (1-y_{ij})log_2(1-P_{ij})$\n",
    " \n",
    "Градиент (производная 1-го порядка потерь относительно параметра модели wk) может быть записан как:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_k} = \\frac{\\partial L}{\\partial s_i} \\frac{\\partial s_i}{\\partial w_k} + \\frac{\\partial L}{\\partial s_j} \\frac{\\partial s_j}{\\partial w_k}$\n",
    "\n",
    "Проще говоря, влияние изменения параметра модели wk будет происходить через результирующие изменения в оценках модели, а затем изменения в потерях. Теперь перепишите градиент общих потерь для всех обучающих пар {i, j}, удовлетворяющих i≻j:\n",
    "\n",
    "$\\begin{align}\n",
    "\\frac{\\partial L_T}{\\partial w_k} \n",
    "&= \\sum_{\\{i, j\\}} \\bigg[ \\frac{\\partial L}{\\partial s_i} \\frac{\\partial s_i}{\\partial w_k} + \\frac{\\partial L}{\\partial s_j} \\frac{\\partial s_j}{\\partial w_k} \\bigg] \\\\ \n",
    "&= \\sum_i \\frac{\\partial s_i}{\\partial w_k} \\bigg( \\sum_{\\forall j \\prec i} \\frac{\\partial L(s_i, s_j)}{\\partial s_i} \\bigg) + \\sum_j \\frac{\\partial s_j}{\\partial w_k} \\bigg( \\sum_{\\forall i \\succ j} \\frac{\\partial L(s_i, s_j)}{\\partial s_j} \\bigg)\n",
    "\\end{align}$\n",
    " \n",
    "при том, что:\n",
    "$\\frac{\\partial L(s_i, s_j)}{\\partial s_i} = - \\frac{\\partial L(s_i, s_j)}{\\partial s_j} = log_2e\\big[(1 - y_{ij}) - \\frac{1}{1 + e^{s_i - s_j}}\\big]$,\n",
    "и переиндексация второго члена, мы получаем:\n",
    "\n",
    "$\\begin{align}\n",
    "\\frac{\\partial L_T}{\\partial w_k} \n",
    "&= \\sum_i \\frac{\\partial s_i}{\\partial w_k} \\bigg[ \\sum_{\\forall j \\prec i} \\frac{\\partial L(s_i, s_j)}{\\partial s_i} + \\sum_{\\forall j \\prec i} \\frac{\\partial L(s_j, s_i)}{\\partial s_i} \\bigg] \\\\\n",
    "&= \\sum_i \\frac{\\partial s_i}{\\partial w_k} \\bigg[ \\sum_{\\forall j \\prec i} \\frac{\\partial L(s_i, s_j)}{\\partial s_i} - \\sum_{\\forall j \\succ i} \\frac{\\partial L(s_j, s_i)}{\\partial s_j} \\bigg] \\\\\n",
    "&= \\sum_i \\frac{\\partial s_i}{\\partial w_k} \\lambda_i\n",
    "\\end{align}$\n",
    "\n",
    "Интуиция за вышеуказанным градиентом:\n",
    "\n",
    "Для каждого документа в данном запросе существует компонент градиента, который мы обозначили как лямбда, который вычисляется путем рассмотрения всех сравниваемых с ним документов высшего и более низкого качества. Относительно худший документ подтолкнет текущий документ вверх, а относительно лучший - вниз.\n",
    "\n",
    "Смысл вышеупомянутой факторизации заключается в том, что в процессе обучения, вместо обновления каждой пары документов, мы можем выполнять обновление для каждого запроса. А поскольку лямбда намного дешевле вычислить, весь процесс обучения может значительно ускориться."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40815661",
   "metadata": {},
   "source": [
    "## Ranking Metric Optimization\n",
    "Поскольку мы моделируем разницу оценок пары документов в запросе как показатель вероятности, модель оптимизирует попарную корректность ранжирования, что может не быть в конечном итоге желаемой целью.\n",
    "\n",
    "Помните, что цель ранжирования действительно измеряется (в идеале) позиционно-зависимым градуированным показателем, таким как NDCG. Но в приведенной выше установке NDCG напрямую не связан с минимизацией кросс-энтропии. Прямым и простым решением является использование NDCG в качестве критерия ранней остановки и определение с помощью набора данных проверки.\n",
    "\n",
    "LambdaRank предлагает еще одно решение. Исследователь обнаружил, что во время обновления градиента с использованием понятия лямбда для каждой пары вместо вычисления только лямбда мы можем скорректировать лямбда путем изменения NDCG для этой пары при условии, что позиции двух элементов поменялись местами друг с другом.\n",
    "\n",
    "Лямбда данного документа:\n",
    "\n",
    "$\\begin{align}\n",
    "\\lambda_i \n",
    "&= \\bigg[ \\sum_{\\forall j \\prec i} \\frac{\\partial L(s_i, s_j)}{\\partial s_i} - \\sum_{\\forall j \\succ i} \\frac{\\partial L(s_j, s_i)}{\\partial s_j} \\bigg] \\\\\n",
    "&= \\bigg[ \\sum_{\\forall j \\prec i} \\lambda_{ij} - \\sum_{\\forall j \\succ i} \\lambda_{ij} \\bigg]\n",
    "\\end{align}$\n",
    "\n",
    "Предлагаемый метод состоит в том, чтобы настроить попарную лямбду λij так, чтобы:\n",
    "\n",
    "$\\lambda_{ij} \\equiv \\frac{\\partial L(s_i, s_j)}{\\partial s_i} \\cdot |\\Delta NDCG_{ij}|$\n",
    "где  ΔNDCGij  это изменение в NDCG, когда позиции i и j меняются местами.\n",
    "\n",
    "Исследователь обнаружил, что при такой корректировке, без теоретических доказательств, модель эмпирически оптимизирует NDCG и, следовательно, дает лучшие общие результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952f58c3",
   "metadata": {},
   "source": [
    "## LambdaMART\n",
    "LambdaMART - это просто LambdaNet, но она заменяет базовую модель нейронной сети **деревьями регрессии с градиентным усилением** (или, в более общем смысле, машинами с градиентным усилением, GBM). GBM доказал свою надежность и производительность при решении реальных проблем.\n",
    "\n",
    "Модель выигрывает несколько реальных крупномасштабных соревнований LTR.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49de92c4",
   "metadata": {},
   "source": [
    "## LambdaLoss\n",
    "В исходной структуре LambdaRank и LambdaMART не проводилось никаких теоретических работ, чтобы математически доказать, что метрика ранжирования оптимизируется после корректировки расчета лямбда. Вывод основан исключительно на эмпирических работах, то есть на наблюдении за результатами из различных наборов данных и моделирования с экспериментами.\n",
    "\n",
    "Исследователи из Google недавно (2018 г.) опубликовали обобщенную структуру под названием LambdaLoss, которая служит расширением исходной модели ранжирования и содержит подробные теоретические обоснования, подтверждающие, что модель действительно оптимизирует метрику ранжирования."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53f3b60",
   "metadata": {},
   "source": [
    "## Implement LambdaMART using lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "706168ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T19:58:23.513635Z",
     "start_time": "2021-11-09T19:58:23.496844Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>62</td>\n",
       "      <td>11089534</td>\n",
       "      <td>2</td>\n",
       "      <td>116</td>\n",
       "      <td>64034</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>54</td>\n",
       "      <td>11089534</td>\n",
       "      <td>2</td>\n",
       "      <td>124</td>\n",
       "      <td>64034</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>3344</td>\n",
       "      <td>14</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>56</td>\n",
       "      <td>11089534</td>\n",
       "      <td>13</td>\n",
       "      <td>123</td>\n",
       "      <td>63933</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>256</td>\n",
       "      <td>49697</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1  2  3  4  5  6    7    8         9  ...  128       129  130  131  \\\n",
       "0  0.50  1  3  3  0  0  3  1.0  1.0  0.000000  ...   62  11089534    2  116   \n",
       "1  0.50  1  3  0  3  0  3  1.0  0.0  1.000000  ...   54  11089534    2  124   \n",
       "2  0.00  1  3  0  2  0  3  1.0  0.0  0.666667  ...   45         3    1  124   \n",
       "3  0.50  1  3  0  3  0  3  1.0  0.0  1.000000  ...   56  11089534   13  123   \n",
       "4  0.25  1  3  0  3  0  3  1.0  0.0  1.000000  ...   64         5    7  256   \n",
       "\n",
       "     132  133  134  135  136  137  \n",
       "0  64034   13    3    0    0  0.0  \n",
       "1  64034    1    2    0    0  0.0  \n",
       "2   3344   14   67    0    0  0.0  \n",
       "3  63933    1    3    0    0  0.0  \n",
       "4  49697    1   13    0    0  0.0  \n",
       "\n",
       "[5 rows x 138 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "346bc67b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T19:58:33.921582Z",
     "start_time": "2021-11-09T19:58:33.904569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>266</td>\n",
       "      <td>25070</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.25</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "      <td>12860</td>\n",
       "      <td>65</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "      <td>1131</td>\n",
       "      <td>112</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.25</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>50</td>\n",
       "      <td>81775</td>\n",
       "      <td>0</td>\n",
       "      <td>560</td>\n",
       "      <td>61224</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57953</td>\n",
       "      <td>15600</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0   1  2  3  4  5  6    7    8    9  ...  128    129  130    131    132  \\\n",
       "0  0.50  13  2  0  2  1  2  1.0  0.0  1.0  ...   35      1    0    266  25070   \n",
       "1  0.25  13  2  0  0  0  2  1.0  0.0  0.0  ...   17     93    0    153  12860   \n",
       "2  0.75  13  2  0  1  0  2  1.0  0.0  0.5  ...   19      0    0    153   1131   \n",
       "3  0.25  13  2  0  2  1  2  1.0  0.0  1.0  ...   50  81775    0    560  61224   \n",
       "4  0.00  13  1  0  0  0  1  0.5  0.0  0.0  ...   24      0    0  57953  15600   \n",
       "\n",
       "   133  134  135  136  137  \n",
       "0   28    7    0    0  0.0  \n",
       "1   65  158    0    0  0.0  \n",
       "2  112  141    0    0  0.0  \n",
       "3    1   14    0    0  0.0  \n",
       "4   15   12    0    0  0.0  \n",
       "\n",
       "[5 rows x 138 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f2500c33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T20:08:42.195426Z",
     "start_time": "2021-11-09T20:08:42.167827Z"
    }
   },
   "outputs": [],
   "source": [
    "#from catboost.datasets import msrank_10k\n",
    "#train_df, test_df = msrank_10k()\n",
    "\n",
    "X_train = train_df.drop(['0', '1'], axis=1).values\n",
    "y_train = train_df['0'].values\n",
    "\n",
    "qids_train = train_df.groupby(\"1\")[\"1\"].count().to_numpy()\n",
    "\n",
    "X_test = test_df.drop(['0', '1'], axis=1).values\n",
    "y_test = test_df['0'].values\n",
    "\n",
    "qids_test = test_df.groupby(\"1\")[\"1\"].count().to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0fc5d7c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T20:08:42.957230Z",
     "start_time": "2021-11-09T20:08:42.954883Z"
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "eecf632c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T20:11:30.731151Z",
     "start_time": "2021-11-09T20:08:51.596013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's ndcg@10: 0.369033\n",
      "[20]\tvalid_0's ndcg@10: 0.391039\n",
      "[30]\tvalid_0's ndcg@10: 0.40436\n",
      "[40]\tvalid_0's ndcg@10: 0.394718\n",
      "[50]\tvalid_0's ndcg@10: 0.405227\n",
      "[60]\tvalid_0's ndcg@10: 0.407002\n",
      "[70]\tvalid_0's ndcg@10: 0.409656\n",
      "[80]\tvalid_0's ndcg@10: 0.405249\n",
      "[90]\tvalid_0's ndcg@10: 0.406912\n",
      "[100]\tvalid_0's ndcg@10: 0.400645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRanker(metric='ndcg', objective='lambdarank')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lgb.LGBMRanker(\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    ")\n",
    "model.fit(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    group=qids_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_group=[qids_test],\n",
    "    eval_at=10,\n",
    "    verbose=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7167ede3",
   "metadata": {},
   "source": [
    "## Listwise LTR на tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ca3d95",
   "metadata": {},
   "source": [
    "Мы будем использовать tensorflow вместе с tensorflow_ranking, чтобы продемонстрировать, как мы можем построить модель PoC LTR в пределах 200 строк кода Python.\n",
    "\n",
    "Обратите внимание, что обычно использование tensorflow требует гораздо больше усилий, поскольку это низкоуровневый фреймворк для моделирования машинного обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9593ef31",
   "metadata": {},
   "source": [
    "Мы рассматриваем задачу ранжирования над ANTIQUE, набором данных с ответами на вопросы. Учитывая запрос и список ответов, цель состоит в том, чтобы максимизировать метрику, связанную с рангом (NDCG).\n",
    "\n",
    "**A Question Answering Dataset**\n",
    "\n",
    "ANTIQUE - это общедоступный набор данных для неактуальных ответов на вопросы в открытом домене, собранный через Yahoo! ответы.\n",
    "\n",
    "На каждый вопрос есть список ответов, актуальность которых оценивается по шкале от 1 до 5.\n",
    "\n",
    "Размер списка может варьироваться в зависимости от запроса, поэтому мы используем фиксированный «размер списка» 50, где список либо усечен, либо дополнен фиктивными значениями.\n",
    "\n",
    "Этот набор данных подходит для сценария обучения ранжированию. Набор данных разделен на 2206 запросов для обучения и 200 запросов для тестирования.\n",
    "\n",
    "Загрузим файл обучения, тестовых данных и словарь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d5da0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "http://hamedz.ir/resources/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d5283017",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:50:04.426447Z",
     "start_time": "2021-11-10T18:50:04.424335Z"
    }
   },
   "outputs": [],
   "source": [
    "#!wget -O \"/tmp/vocab.txt\" \"http://ciir.cs.umass.edu/downloads/Antique/tf-ranking/vocab.txt\"\n",
    "#!wget -O \"/tmp/train.tfrecords\" \"http://ciir.cs.umass.edu/downloads/Antique/tf-ranking/ELWC/train.tfrecords\"\n",
    "#!wget -O \"/tmp/test.tfrecords\" \"http://ciir.cs.umass.edu/downloads/Antique/tf-ranking//ELWC/test.tfrecords\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37af753e",
   "metadata": {},
   "source": [
    "## Data Formats for Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c825ba",
   "metadata": {},
   "source": [
    "Для представления данных ранжирования протобуферы представляют собой расширяемые структуры, подходящие для хранения данных в сериализованном формате либо локально, либо распределенно.\n",
    "\n",
    "https://chromium.googlesource.com/external/github.com/tensorflow/tensorflow/+/r0.10/tensorflow/g3doc/how_tos/tool_developers/index.md\n",
    "\n",
    "Ранжирование обычно состоит из характеристик, соответствующих каждому из сортируемых примеров. Кроме того, для ранжирования полезны признаки, связанные с запросом, пользователем или сессии. Мы называем их признаками контекста, поскольку они не зависят от примеров.\n",
    "\n",
    "Мы используем популярный протокол tf.Example для представления признаков для контекста и каждого из примеров. Мы используем протобуфер ExampleListWithContext (ELWC) для хранения контекста как tf.Example и списка примеров, которые будут ранжироваться как список протоколов tf.Example.\n",
    "\n",
    "Здесь определяется протбуфер ExampleListWithContext.\n",
    "\n",
    "Создадим фиктивные данные в формате ELWC. Мы будем использовать эти фиктивные данные, чтобы показать, как выглядит прототип.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ad7345",
   "metadata": {},
   "source": [
    "Загрузите и установите пакеты TensorFlow Ranking и TensorFlow Serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc2c4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bba095c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T20:13:02.577036Z",
     "start_time": "2021-11-10T20:13:00.367573Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-10 23:13:00.517134: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-10 23:13:00.517175: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/data/home/irsafilo/course/shad/venv/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.4.0 and strictly below 2.7.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.7.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "from tensorflow_serving.apis import input_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c870762e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "046f26fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T20:13:10.430285Z",
     "start_time": "2021-11-10T20:13:10.426744Z"
    }
   },
   "outputs": [],
   "source": [
    "from google.protobuf import text_format\n",
    "\n",
    "CONTEXT = text_format.Parse(\n",
    "    \"\"\"\n",
    "    features {\n",
    "      feature {\n",
    "        key: \"query_tokens\"\n",
    "        value { bytes_list { value: [\"this\", \"is\", \"a\", \"relevant\", \"question\"] } }\n",
    "      }\n",
    "    }\"\"\", tf.train.Example())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94129f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14475852",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T20:13:17.957907Z",
     "start_time": "2021-11-10T20:13:17.954278Z"
    }
   },
   "outputs": [],
   "source": [
    "EXAMPLES = [\n",
    "    text_format.Parse(\n",
    "    \"\"\"\n",
    "    features {\n",
    "      feature {\n",
    "        key: \"document_tokens\"\n",
    "        value { bytes_list { value: [\"this\", \"is\", \"a\", \"relevant\", \"answer\"] } }\n",
    "      }\n",
    "      feature {\n",
    "        key: \"relevance\"\n",
    "        value { int64_list { value: 5 } }\n",
    "      }\n",
    "    }\"\"\", tf.train.Example()),\n",
    "    text_format.Parse(\n",
    "        \"\"\"\n",
    "    features {\n",
    "      feature {\n",
    "        key: \"document_tokens\"\n",
    "        value { bytes_list { value: [\"irrelevant\", \"data\"] } }\n",
    "      }\n",
    "      feature {\n",
    "        key: \"relevance\"\n",
    "        value { int64_list { value: 1 } }\n",
    "      }\n",
    "    }\"\"\", tf.train.Example()),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd81f0e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T20:13:21.434437Z",
     "start_time": "2021-11-10T20:13:21.431332Z"
    }
   },
   "outputs": [],
   "source": [
    "ELWC = input_pb2.ExampleListWithContext()\n",
    "ELWC.context.CopyFrom(CONTEXT)\n",
    "for example in EXAMPLES:\n",
    "    example_features = ELWC.examples.add()\n",
    "    example_features.CopyFrom(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a84efc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T20:13:24.666476Z",
     "start_time": "2021-11-10T20:13:24.662982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples {\n",
      "  features {\n",
      "    feature {\n",
      "      key: \"document_tokens\"\n",
      "      value {\n",
      "        bytes_list {\n",
      "          value: \"this\"\n",
      "          value: \"is\"\n",
      "          value: \"a\"\n",
      "          value: \"relevant\"\n",
      "          value: \"answer\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    feature {\n",
      "      key: \"relevance\"\n",
      "      value {\n",
      "        int64_list {\n",
      "          value: 5\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "examples {\n",
      "  features {\n",
      "    feature {\n",
      "      key: \"document_tokens\"\n",
      "      value {\n",
      "        bytes_list {\n",
      "          value: \"irrelevant\"\n",
      "          value: \"data\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    feature {\n",
      "      key: \"relevance\"\n",
      "      value {\n",
      "        int64_list {\n",
      "          value: 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "context {\n",
      "  features {\n",
      "    feature {\n",
      "      key: \"query_tokens\"\n",
      "      value {\n",
      "        bytes_list {\n",
      "          value: \"this\"\n",
      "          value: \"is\"\n",
      "          value: \"a\"\n",
      "          value: \"relevant\"\n",
      "          value: \"question\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ELWC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd33677",
   "metadata": {},
   "source": [
    "\n",
    "**Dependencies and Global Variables**\n",
    "\n",
    "Здесь мы определяем пути обучения и тестирования, а также гиперпараметры модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70b736f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:30:52.894467Z",
     "start_time": "2021-11-10T18:30:52.890257Z"
    }
   },
   "outputs": [],
   "source": [
    "# Store the paths to files containing training and test instances.\n",
    "_TRAIN_DATA_PATH = \"train.tfrecords\"\n",
    "_TEST_DATA_PATH = \"test.tfrecords\"\n",
    "\n",
    "# Store the vocabulary path for query and document tokens.\n",
    "_VOCAB_PATH = \"vocab.txt\"\n",
    "\n",
    "# The maximum number of documents per query in the dataset.\n",
    "# Document lists are padded or truncated to this size.\n",
    "_LIST_SIZE = 50\n",
    "\n",
    "# The document relevance label.\n",
    "_LABEL_FEATURE = \"relevance\"\n",
    "\n",
    "# Padding labels are set negative so that the corresponding examples can be\n",
    "# ignored in loss and metrics.\n",
    "_PADDING_LABEL = -1\n",
    "\n",
    "# Learning rate for optimizer.\n",
    "_LEARNING_RATE = 0.05\n",
    "\n",
    "# Parameters to the scoring function.\n",
    "_BATCH_SIZE = 32\n",
    "_HIDDEN_LAYER_DIMS = [\"64\", \"32\", \"16\"]\n",
    "_DROPOUT_RATE = 0.8\n",
    "_GROUP_SIZE = 1  # Pointwise scoring.\n",
    "\n",
    "# Location of model directory and number of training steps.\n",
    "_MODEL_DIR = \"ranking_model_dir\"\n",
    "_NUM_TRAIN_STEPS = 15 * 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bc8cf6",
   "metadata": {},
   "source": [
    "**Components of a Ranking Estimator**\n",
    "\n",
    "Общие компоненты рейтингового оценщика показаны ниже.\n",
    "\n",
    "Ключевые компоненты библиотеки:\n",
    "\n",
    "- Input Reader\n",
    "- Tranform Function\n",
    "- Scoring Function\n",
    "- Ranking Losses\n",
    "- Ranking Metrics\n",
    "- Ranking Head\n",
    "- Model Builder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc26865",
   "metadata": {},
   "source": [
    "Столбцы фичей - это абстракции TensorFlow, которые используются для сбора обширной информации о каждой фичи. Это позволяет легко преобразовывать широкий спектр необработанных фичей и взаимодействовать с ними в одном обьекте.\n",
    "\n",
    "В соответствии с нашими форматами ввода для ранжирования, такими как формат ELWC, мы создаем столбцы фичей для всех признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "310e2194",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T20:15:56.740566Z",
     "start_time": "2021-11-10T20:15:56.736755Z"
    }
   },
   "outputs": [],
   "source": [
    "_EMBEDDING_DIMENSION = 20\n",
    "\n",
    "\n",
    "def context_feature_columns():\n",
    "    \"\"\"Returns context feature names to column definitions.\"\"\"\n",
    "    sparse_column = tf.feature_column.categorical_column_with_vocabulary_file(\n",
    "    key=\"query_tokens\",\n",
    "    vocabulary_file=_VOCAB_PATH)\n",
    "    query_embedding_column = tf.feature_column.embedding_column(\n",
    "    sparse_column, _EMBEDDING_DIMENSION)\n",
    "    return {\"query_tokens\": query_embedding_column}\n",
    "\n",
    "\n",
    "def example_feature_columns():\n",
    "    \"\"\"Returns the example feature columns.\"\"\"\n",
    "    sparse_column = tf.feature_column.categorical_column_with_vocabulary_file(\n",
    "    key=\"document_tokens\",\n",
    "    vocabulary_file=_VOCAB_PATH)\n",
    "    document_embedding_column = tf.feature_column.embedding_column(\n",
    "    sparse_column, _EMBEDDING_DIMENSION)\n",
    "    return {\"document_tokens\": document_embedding_column}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859cb911",
   "metadata": {},
   "source": [
    "### Reading Input Data using input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae0e3e9",
   "metadata": {},
   "source": [
    "Устройство чтения ввода считывает данные из постоянного хранилища для создания необработанных плотных и разреженных тензоров соответствующего типа для каждой фичи. Примеры признаков представлены трехмерными тензорами (где измерения соответствуют запросам, примерам и значениям признака). Особенности контекста представлены двумерными тензорами (где измерения соответствуют запросам и значениям признаков)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170e6aea",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials/structured_data/feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd670f90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T20:16:59.078026Z",
     "start_time": "2021-11-10T20:16:59.073548Z"
    }
   },
   "outputs": [],
   "source": [
    "def input_fn(path, num_epochs=None):\n",
    "    context_feature_spec = tf.feature_column.make_parse_example_spec(\n",
    "        context_feature_columns().values())\n",
    "    label_column = tf.feature_column.numeric_column(\n",
    "        _LABEL_FEATURE, dtype=tf.int64, default_value=_PADDING_LABEL)\n",
    "    example_feature_spec = tf.feature_column.make_parse_example_spec(\n",
    "        list(example_feature_columns().values()) + [label_column])\n",
    "    dataset = tfr.data.build_ranking_dataset(\n",
    "        file_pattern=path,\n",
    "        data_format=tfr.data.ELWC,\n",
    "        batch_size=_BATCH_SIZE,\n",
    "        list_size=_LIST_SIZE,\n",
    "        context_feature_spec=context_feature_spec,\n",
    "        example_feature_spec=example_feature_spec,\n",
    "        reader=tf.data.TFRecordDataset,\n",
    "        shuffle=False,\n",
    "        num_epochs=num_epochs)\n",
    "    features = tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n",
    "    label = tf.squeeze(features.pop(_LABEL_FEATURE), axis=2)\n",
    "    label = tf.cast(label, tf.float32)\n",
    "\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c528dd4",
   "metadata": {},
   "source": [
    "**Feature Transformations with transform_fn**\n",
    "\n",
    "Функция преобразования принимает необработанные dense or sparse features из устройства чтения ввода, применяет подходящие преобразования для возврата плотных представлений для каждой фичи. Это важно перед передачей этих признаков в нейронную сеть, поскольку слои нейронных сетей обычно принимают dense features в качестве входных данных.\n",
    "\n",
    "Функция преобразования обрабатывает любые преобразования признаков, определенные пользователем. Для работы с разреженными признаками, такими как текстовые данные, мы предоставляем удобную возможность для создания общих встраиваний на основе столбцов фичей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "912d1444",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T19:03:38.125052Z",
     "start_time": "2021-11-10T19:03:38.121879Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_transform_fn():\n",
    "    def _transform_fn(features, mode):\n",
    "        \"\"\"Defines transform_fn.\"\"\"\n",
    "        context_features, example_features = tfr.feature.encode_listwise_features(\n",
    "        features=features,\n",
    "        context_feature_columns=context_feature_columns(),\n",
    "        example_feature_columns=example_feature_columns(),\n",
    "        mode=mode,\n",
    "        scope=\"transform_layer\")\n",
    "\n",
    "        return context_features, example_features\n",
    "    return _transform_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282eb7fd",
   "metadata": {},
   "source": [
    "**Feature Interactions using scoring_fn**\n",
    "Затем мы переходим к функции оценки, которая, возможно, лежит в основе модели рейтинга TF. Идея состоит в том, чтобы вычислить оценку релевантности для (набора) пар запрос-документ. Модель TF-Ranking будет использовать данные обучения для изучения этой функции.\n",
    "\n",
    "Здесь мы формулируем функцию оценки с использованием сети прямого распространения. Функция использует характеристики одного примера (например, пары запрос-документ) и выдает оценку релевантности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "36d3ffa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:37:50.093115Z",
     "start_time": "2021-11-10T18:37:50.086509Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_score_fn():\n",
    "    \"\"\"Returns a scoring function to build `EstimatorSpec`.\"\"\"\n",
    "\n",
    "    def _score_fn(context_features, group_features, mode, params, config):\n",
    "        \"\"\"Defines the network to score a group of documents.\"\"\"\n",
    "        with tf.compat.v1.name_scope(\"input_layer\"):\n",
    "            context_input = [\n",
    "              tf.compat.v1.layers.flatten(context_features[name])\n",
    "            for name in sorted(context_feature_columns())\n",
    "            ]\n",
    "            group_input = [\n",
    "              tf.compat.v1.layers.flatten(group_features[name])\n",
    "              for name in sorted(example_feature_columns())\n",
    "            ]\n",
    "            input_layer = tf.concat(context_input + group_input, 1)\n",
    "\n",
    "        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "        cur_layer = input_layer\n",
    "        cur_layer = tf.compat.v1.layers.batch_normalization(\n",
    "        cur_layer,\n",
    "        training=is_training,\n",
    "        momentum=0.99)\n",
    "\n",
    "        for i, layer_width in enumerate(int(d) for d in _HIDDEN_LAYER_DIMS):\n",
    "            cur_layer = tf.compat.v1.layers.dense(cur_layer, units=layer_width)\n",
    "            cur_layer = tf.compat.v1.layers.batch_normalization(\n",
    "            cur_layer,\n",
    "            training=is_training,\n",
    "            momentum=0.99)\n",
    "        cur_layer = tf.nn.relu(cur_layer)\n",
    "        cur_layer = tf.compat.v1.layers.dropout(\n",
    "          inputs=cur_layer, rate=_DROPOUT_RATE, training=is_training)\n",
    "        logits = tf.compat.v1.layers.dense(cur_layer, units=_GROUP_SIZE)\n",
    "        return logits\n",
    "\n",
    "    return _score_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d51fbb3",
   "metadata": {},
   "source": [
    "## Losses, Metrics and Ranking Head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab520ec2",
   "metadata": {},
   "source": [
    "**Evaluation Metrics**\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc90401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d028a0ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T19:06:17.656598Z",
     "start_time": "2021-11-10T19:06:17.653131Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_metric_fns():\n",
    "    \"\"\"Returns a dict from name to metric functions.\n",
    "\n",
    "  This can be customized as follows. Care must be taken when handling padded\n",
    "  lists.\n",
    "\n",
    "  def _auc(labels, predictions, features):\n",
    "    is_label_valid = tf_reshape(tf.greater_equal(labels, 0.), [-1, 1])\n",
    "    clean_labels = tf.boolean_mask(tf.reshape(labels, [-1, 1], is_label_valid)\n",
    "    clean_pred = tf.boolean_maks(tf.reshape(predictions, [-1, 1], is_label_valid)\n",
    "    return tf.metrics.auc(clean_labels, tf.sigmoid(clean_pred), ...)\n",
    "  metric_fns[\"auc\"] = _auc\n",
    "\n",
    "  Returns:\n",
    "    A dict mapping from metric name to a metric function with above signature.\n",
    "  \"\"\"\n",
    "    metric_fns = {}\n",
    "    metric_fns.update({\n",
    "      f\"metric/ndcg@{topn}\": tfr.metrics.make_ranking_metric_fn(\n",
    "          tfr.metrics.RankingMetricKey.NDCG, topn=topn)\n",
    "      for topn in [1, 3, 5, 10]\n",
    "    })\n",
    "\n",
    "    return metric_fns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d3ed7",
   "metadata": {},
   "source": [
    "### Ranking Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e25864dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:38:25.082549Z",
     "start_time": "2021-11-10T18:38:25.079743Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a loss function. To find a complete list of available\n",
    "# loss functions or to learn how to add your own custom function\n",
    "# please refer to the tensorflow_ranking.losses module.\n",
    "\n",
    "_LOSS = tfr.losses.RankingLossKey.APPROX_NDCG_LOSS\n",
    "loss_fn = tfr.losses.make_loss_fn(_LOSS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fee5fc",
   "metadata": {},
   "source": [
    "### Ranking Head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06516168",
   "metadata": {},
   "source": [
    "В рабочем процессе оценщика Head - это абстракция, которая инкапсулирует потери и соответствующие показатели. Head легко взаимодействует с оценщиком, что требует от пользователя определения функции оценки и определения потерь и расчета показателей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e93aa855",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T19:09:43.901819Z",
     "start_time": "2021-11-10T19:09:43.897858Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdagradOptimizer(\n",
    "learning_rate=_LEARNING_RATE)\n",
    "\n",
    "\n",
    "def _train_op_fn(loss):\n",
    "    \"\"\"Defines train op used in ranking head.\"\"\"\n",
    "    update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n",
    "    minimize_op = optimizer.minimize(\n",
    "      loss=loss, global_step=tf.compat.v1.train.get_global_step())\n",
    "    train_op = tf.group([update_ops, minimize_op])\n",
    "    return train_op\n",
    "\n",
    "\n",
    "ranking_head = tfr.head.create_ranking_head(\n",
    "      loss_fn=loss_fn,\n",
    "      eval_metric_fns=eval_metric_fns(),\n",
    "      train_op_fn=_train_op_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4077559a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73e5f4ec",
   "metadata": {},
   "source": [
    "### Putting It All Together in a Model Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5a5b2d",
   "metadata": {},
   "source": [
    "Теперь мы готовы собрать все вышеперечисленные компоненты вместе и создать одну сущность модели, который можно использовать для обучения и оценки модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "18bf38e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:39:03.457985Z",
     "start_time": "2021-11-10T18:39:03.454268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Building groupwise ranking model.\n"
     ]
    }
   ],
   "source": [
    "model_fn = tfr.model.make_groupwise_ranking_fn(\n",
    "          group_score_fn=make_score_fn(),\n",
    "          transform_fn=make_transform_fn(),\n",
    "          group_size=_GROUP_SIZE,\n",
    "          ranking_head=ranking_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add894c8",
   "metadata": {},
   "source": [
    "### Train and evaluate the ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "de43b0d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:39:26.948966Z",
     "start_time": "2021-11-10T18:39:26.945065Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_eval_fn():\n",
    "    \"\"\"Train and eval function used by `tf.estimator.train_and_evaluate`.\"\"\"\n",
    "    run_config = tf.estimator.RunConfig(\n",
    "      save_checkpoints_steps=1000)\n",
    "    ranker = tf.estimator.Estimator(\n",
    "      model_fn=model_fn,\n",
    "      model_dir=_MODEL_DIR,\n",
    "      config=run_config)\n",
    "\n",
    "    train_input_fn = lambda: input_fn(_TRAIN_DATA_PATH)\n",
    "    eval_input_fn = lambda: input_fn(_TEST_DATA_PATH, num_epochs=1)\n",
    "\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "      input_fn=train_input_fn, max_steps=_NUM_TRAIN_STEPS)\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "      name=\"eval\",\n",
    "      input_fn=eval_input_fn,\n",
    "      throttle_secs=15)\n",
    "    return (ranker, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1795ca00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T18:43:56.123952Z",
     "start_time": "2021-11-10T18:40:48.763470Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'ranking_model_dir', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:Estimator's model_fn (<function _make_model_fn.<locals>._model_fn at 0x7fc9bda8ca60>) includes params argument, but params are not passed to Estimator.\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1000 or save_checkpoints_secs None.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "WARNING:tensorflow:From /data/home/irsafilo/course/shad/venv/lib/python3.7/site-packages/tensorflow/python/training/adagrad.py:143: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-10 21:40:52.340298: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-10 21:40:52.669178: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ranking_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
      "INFO:tensorflow:loss = -0.8254502, step = 0\n",
      "INFO:tensorflow:global_step/sec: 74.6338\n",
      "INFO:tensorflow:loss = -0.8395604, step = 100 (1.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.781\n",
      "INFO:tensorflow:loss = -0.8433441, step = 200 (0.974 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.27\n",
      "INFO:tensorflow:loss = -0.8399544, step = 300 (0.977 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.538\n",
      "INFO:tensorflow:loss = -0.81956446, step = 400 (0.966 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.412\n",
      "INFO:tensorflow:loss = -0.8653097, step = 500 (0.958 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.103\n",
      "INFO:tensorflow:loss = -0.8816631, step = 600 (0.951 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.614\n",
      "INFO:tensorflow:loss = -0.84986377, step = 700 (0.965 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.097\n",
      "INFO:tensorflow:loss = -0.90473086, step = 800 (0.961 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.412\n",
      "INFO:tensorflow:loss = -0.81029063, step = 900 (0.940 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1000...\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into ranking_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1000...\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-11-10T21:41:09\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ranking_model_dir/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-10 21:41:09.287311: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Inference Time : 0.76280s\n",
      "INFO:tensorflow:Finished evaluation at 2021-11-10-21:41:09\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, labels_mean = 1.9630322, logits_mean = 0.9682748, loss = -0.85726124, metric/ndcg@1 = 0.60714287, metric/ndcg@10 = 0.80985826, metric/ndcg@3 = 0.69060755, metric/ndcg@5 = 0.7436608\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: ranking_model_dir/model.ckpt-1000\n",
      "INFO:tensorflow:global_step/sec: 15.3705\n",
      "INFO:tensorflow:loss = -0.86674887, step = 1000 (6.505 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.608\n",
      "INFO:tensorflow:loss = -0.89810175, step = 1100 (0.929 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.152\n",
      "INFO:tensorflow:loss = -0.864715, step = 1200 (0.934 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.705\n",
      "INFO:tensorflow:loss = -0.84928954, step = 1300 (0.955 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.625\n",
      "INFO:tensorflow:loss = -0.86609375, step = 1400 (0.938 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.9652\n",
      "INFO:tensorflow:loss = -0.8766582, step = 1500 (1.010 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.221\n",
      "INFO:tensorflow:loss = -0.88955283, step = 1600 (0.950 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.213\n",
      "INFO:tensorflow:loss = -0.84866124, step = 1700 (0.969 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.575\n",
      "INFO:tensorflow:loss = -0.85966974, step = 1800 (0.966 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.5489\n",
      "INFO:tensorflow:loss = -0.88983524, step = 1900 (1.004 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 2000...\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into ranking_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 2000...\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-11-10T21:41:24\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ranking_model_dir/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-10 21:41:24.369335: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Inference Time : 0.77016s\n",
      "INFO:tensorflow:Finished evaluation at 2021-11-10-21:41:25\n",
      "INFO:tensorflow:Saving dict for global step 2000: global_step = 2000, labels_mean = 1.9630322, logits_mean = 1.1032636, loss = -0.8568441, metric/ndcg@1 = 0.62428576, metric/ndcg@10 = 0.8177373, metric/ndcg@3 = 0.7033743, metric/ndcg@5 = 0.7513013\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2000: ranking_model_dir/model.ckpt-2000\n",
      "INFO:tensorflow:global_step/sec: 23.8324\n",
      "INFO:tensorflow:loss = -0.88623816, step = 2000 (4.196 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.35\n",
      "INFO:tensorflow:loss = -0.8666156, step = 2100 (0.977 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.719\n",
      "INFO:tensorflow:loss = -0.8971752, step = 2200 (0.993 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.882\n",
      "INFO:tensorflow:loss = -0.9024274, step = 2300 (1.022 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.6727\n",
      "INFO:tensorflow:loss = -0.87781584, step = 2400 (1.003 sec)\n",
      "INFO:tensorflow:global_step/sec: 96.4931\n",
      "INFO:tensorflow:loss = -0.8713346, step = 2500 (1.036 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.6357\n",
      "INFO:tensorflow:loss = -0.87129796, step = 2600 (1.057 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.7151\n",
      "INFO:tensorflow:loss = -0.87868583, step = 2700 (1.014 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.4003\n",
      "INFO:tensorflow:loss = -0.9206599, step = 2800 (1.016 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.217\n",
      "INFO:tensorflow:loss = -0.88465464, step = 2900 (0.988 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 3000...\n",
      "INFO:tensorflow:Saving checkpoints for 3000 into ranking_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 3000...\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (15 secs).\n",
      "INFO:tensorflow:global_step/sec: 49.2416\n",
      "INFO:tensorflow:loss = -0.8679372, step = 3000 (2.031 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.688\n",
      "INFO:tensorflow:loss = -0.87130785, step = 3100 (0.956 sec)\n",
      "INFO:tensorflow:global_step/sec: 109.188\n",
      "INFO:tensorflow:loss = -0.87579626, step = 3200 (0.915 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.581\n",
      "INFO:tensorflow:loss = -0.8715092, step = 3300 (0.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.22\n",
      "INFO:tensorflow:loss = -0.87801963, step = 3400 (0.908 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.06\n",
      "INFO:tensorflow:loss = -0.90618813, step = 3500 (0.942 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.794\n",
      "INFO:tensorflow:loss = -0.90249, step = 3600 (0.919 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.845\n",
      "INFO:tensorflow:loss = -0.91150045, step = 3700 (0.927 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.368\n",
      "INFO:tensorflow:loss = -0.87416935, step = 3800 (0.923 sec)\n",
      "INFO:tensorflow:global_step/sec: 111.845\n",
      "INFO:tensorflow:loss = -0.88001406, step = 3900 (0.894 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 4000...\n",
      "INFO:tensorflow:Saving checkpoints for 4000 into ranking_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 4000...\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-11-10T21:41:47\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ranking_model_dir/model.ckpt-4000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-10 21:41:47.947945: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Inference Time : 0.99697s\n",
      "INFO:tensorflow:Finished evaluation at 2021-11-10-21:41:48\n",
      "INFO:tensorflow:Saving dict for global step 4000: global_step = 4000, labels_mean = 1.9630322, logits_mean = 1.246339, loss = -0.85561514, metric/ndcg@1 = 0.6464286, metric/ndcg@10 = 0.8250308, metric/ndcg@3 = 0.71171546, metric/ndcg@5 = 0.7571299\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4000: ranking_model_dir/model.ckpt-4000\n",
      "INFO:tensorflow:global_step/sec: 22.8067\n",
      "INFO:tensorflow:loss = -0.9002133, step = 4000 (4.384 sec)\n",
      "INFO:tensorflow:global_step/sec: 112.942\n",
      "INFO:tensorflow:loss = -0.89176995, step = 4100 (0.886 sec)\n",
      "INFO:tensorflow:global_step/sec: 111.936\n",
      "INFO:tensorflow:loss = -0.8835182, step = 4200 (0.893 sec)\n",
      "INFO:tensorflow:global_step/sec: 113.642\n",
      "INFO:tensorflow:loss = -0.9086181, step = 4300 (0.880 sec)\n",
      "INFO:tensorflow:global_step/sec: 112.786\n",
      "INFO:tensorflow:loss = -0.87690514, step = 4400 (0.887 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.514\n",
      "INFO:tensorflow:loss = -0.8822607, step = 4500 (0.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 109.088\n",
      "INFO:tensorflow:loss = -0.8926395, step = 4600 (0.917 sec)\n",
      "INFO:tensorflow:global_step/sec: 109.885\n",
      "INFO:tensorflow:loss = -0.9194852, step = 4700 (0.910 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.692\n",
      "INFO:tensorflow:loss = -0.93122846, step = 4800 (0.955 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.178\n",
      "INFO:tensorflow:loss = -0.8743238, step = 4900 (0.951 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 5000...\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into ranking_model_dir/model.ckpt.\n",
      "WARNING:tensorflow:From /data/home/irsafilo/course/shad/venv/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1058: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 5000...\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (15 secs).\n",
      "INFO:tensorflow:global_step/sec: 50.9525\n",
      "INFO:tensorflow:loss = -0.8977592, step = 5000 (1.963 sec)\n",
      "INFO:tensorflow:global_step/sec: 113.727\n",
      "INFO:tensorflow:loss = -0.88021404, step = 5100 (0.879 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.55\n",
      "INFO:tensorflow:loss = -0.88869715, step = 5200 (0.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.414\n",
      "INFO:tensorflow:loss = -0.91674566, step = 5300 (0.940 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.445\n",
      "INFO:tensorflow:loss = -0.90936834, step = 5400 (0.967 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.027\n",
      "INFO:tensorflow:loss = -0.89364195, step = 5500 (0.934 sec)\n",
      "INFO:tensorflow:global_step/sec: 109.195\n",
      "INFO:tensorflow:loss = -0.9070318, step = 5600 (0.916 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.771\n",
      "INFO:tensorflow:loss = -0.92212135, step = 5700 (0.964 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.567\n",
      "INFO:tensorflow:loss = -0.9294288, step = 5800 (0.948 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.342\n",
      "INFO:tensorflow:loss = -0.8773058, step = 5900 (0.931 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 6000...\n",
      "INFO:tensorflow:Saving checkpoints for 6000 into ranking_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 6000...\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-11-10T21:42:10\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ranking_model_dir/model.ckpt-6000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-10 21:42:10.903825: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Inference Time : 0.76086s\n",
      "INFO:tensorflow:Finished evaluation at 2021-11-10-21:42:11\n",
      "INFO:tensorflow:Saving dict for global step 6000: global_step = 6000, labels_mean = 1.9630322, logits_mean = 1.4057561, loss = -0.86074245, metric/ndcg@1 = 0.6528571, metric/ndcg@10 = 0.8255662, metric/ndcg@3 = 0.7156479, metric/ndcg@5 = 0.7573444\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 6000: ranking_model_dir/model.ckpt-6000\n",
      "INFO:tensorflow:global_step/sec: 23.8706\n",
      "INFO:tensorflow:loss = -0.9010929, step = 6000 (4.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.278\n",
      "INFO:tensorflow:loss = -0.8948064, step = 6100 (0.924 sec)\n",
      "INFO:tensorflow:global_step/sec: 109.137\n",
      "INFO:tensorflow:loss = -0.9220096, step = 6200 (0.916 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.847\n",
      "INFO:tensorflow:loss = -0.88979805, step = 6300 (0.964 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.907\n",
      "INFO:tensorflow:loss = -0.85965335, step = 6400 (0.972 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.641\n",
      "INFO:tensorflow:loss = -0.8781047, step = 6500 (0.929 sec)\n",
      "INFO:tensorflow:global_step/sec: 111.253\n",
      "INFO:tensorflow:loss = -0.8963576, step = 6600 (0.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 111.543\n",
      "INFO:tensorflow:loss = -0.8796567, step = 6700 (0.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.821\n",
      "INFO:tensorflow:loss = -0.9137348, step = 6800 (0.919 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.987\n",
      "INFO:tensorflow:loss = -0.9118257, step = 6900 (0.918 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 7000...\n",
      "INFO:tensorflow:Saving checkpoints for 7000 into ranking_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 7000...\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (15 secs).\n",
      "INFO:tensorflow:global_step/sec: 51.3064\n",
      "INFO:tensorflow:loss = -0.8905644, step = 7000 (1.949 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.618\n",
      "INFO:tensorflow:loss = -0.87937516, step = 7100 (0.985 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.703\n",
      "INFO:tensorflow:loss = -0.8592638, step = 7200 (0.956 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.169\n",
      "INFO:tensorflow:loss = -0.8744334, step = 7300 (0.924 sec)\n",
      "INFO:tensorflow:global_step/sec: 109.024\n",
      "INFO:tensorflow:loss = -0.8931318, step = 7400 (0.918 sec)\n",
      "INFO:tensorflow:global_step/sec: 111.112\n",
      "INFO:tensorflow:loss = -0.8981199, step = 7500 (0.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.819\n",
      "INFO:tensorflow:loss = -0.87424946, step = 7600 (0.919 sec)\n",
      "INFO:tensorflow:global_step/sec: 112.256\n",
      "INFO:tensorflow:loss = -0.8984847, step = 7700 (0.891 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.062\n",
      "INFO:tensorflow:loss = -0.8987565, step = 7800 (0.925 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.026\n",
      "INFO:tensorflow:loss = -0.8783673, step = 7900 (0.943 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 8000...\n",
      "INFO:tensorflow:Saving checkpoints for 8000 into ranking_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 8000...\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-11-10T21:42:34\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ranking_model_dir/model.ckpt-8000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-10 21:42:34.091877: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Inference Time : 0.81218s\n",
      "INFO:tensorflow:Finished evaluation at 2021-11-10-21:42:34\n",
      "INFO:tensorflow:Saving dict for global step 8000: global_step = 8000, labels_mean = 1.9630322, logits_mean = 1.2556727, loss = -0.8628902, metric/ndcg@1 = 0.6571429, metric/ndcg@10 = 0.82717633, metric/ndcg@3 = 0.7141511, metric/ndcg@5 = 0.759437\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 8000: ranking_model_dir/model.ckpt-8000\n",
      "INFO:tensorflow:global_step/sec: 21.7792\n",
      "INFO:tensorflow:loss = -0.8955289, step = 8000 (4.591 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.686\n",
      "INFO:tensorflow:loss = -0.9019767, step = 8100 (0.920 sec)\n",
      "INFO:tensorflow:global_step/sec: 109.06\n",
      "INFO:tensorflow:loss = -0.8938798, step = 8200 (0.917 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.437\n",
      "INFO:tensorflow:loss = -0.91068137, step = 8300 (0.922 sec)\n",
      "INFO:tensorflow:global_step/sec: 109.86\n",
      "INFO:tensorflow:loss = -0.89888, step = 8400 (0.910 sec)\n",
      "INFO:tensorflow:global_step/sec: 111.764\n",
      "INFO:tensorflow:loss = -0.9082114, step = 8500 (0.895 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.239\n",
      "INFO:tensorflow:loss = -0.8604052, step = 8600 (0.924 sec)\n",
      "INFO:tensorflow:global_step/sec: 111.794\n",
      "INFO:tensorflow:loss = -0.88130224, step = 8700 (0.895 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.739\n",
      "INFO:tensorflow:loss = -0.9083393, step = 8800 (0.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.071\n",
      "INFO:tensorflow:loss = -0.8909962, step = 8900 (0.909 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 9000...\n",
      "INFO:tensorflow:Saving checkpoints for 9000 into ranking_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 9000...\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (15 secs).\n",
      "INFO:tensorflow:global_step/sec: 50.2548\n",
      "INFO:tensorflow:loss = -0.90634406, step = 9000 (1.990 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.167\n",
      "INFO:tensorflow:loss = -0.8750572, step = 9100 (0.960 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.93\n",
      "INFO:tensorflow:loss = -0.9077349, step = 9200 (0.981 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.332\n",
      "INFO:tensorflow:loss = -0.8963099, step = 9300 (0.949 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.936\n",
      "INFO:tensorflow:loss = -0.89727676, step = 9400 (0.962 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.748\n",
      "INFO:tensorflow:loss = -0.92801815, step = 9500 (0.937 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.197\n",
      "INFO:tensorflow:loss = -0.90114987, step = 9600 (0.969 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.254\n",
      "INFO:tensorflow:loss = -0.9079529, step = 9700 (0.942 sec)\n",
      "INFO:tensorflow:global_step/sec: 111.812\n",
      "INFO:tensorflow:loss = -0.92175424, step = 9800 (0.894 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.188\n",
      "INFO:tensorflow:loss = -0.91338, step = 9900 (0.933 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 10000...\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into ranking_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 10000...\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-11-10T21:42:56\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ranking_model_dir/model.ckpt-10000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-10 21:42:57.026670: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Inference Time : 0.75210s\n",
      "INFO:tensorflow:Finished evaluation at 2021-11-10-21:42:57\n",
      "INFO:tensorflow:Saving dict for global step 10000: global_step = 10000, labels_mean = 1.9630322, logits_mean = 1.5189284, loss = -0.8644729, metric/ndcg@1 = 0.655, metric/ndcg@10 = 0.830528, metric/ndcg@3 = 0.7181787, metric/ndcg@5 = 0.7597439\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 10000: ranking_model_dir/model.ckpt-10000\n",
      "INFO:tensorflow:global_step/sec: 23.9868\n",
      "INFO:tensorflow:loss = -0.89392644, step = 10000 (4.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 111.435\n",
      "INFO:tensorflow:loss = -0.88195837, step = 10100 (0.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.342\n",
      "INFO:tensorflow:loss = -0.91282356, step = 10200 (0.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 111.23\n",
      "INFO:tensorflow:loss = -0.9189636, step = 10300 (0.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 109.271\n",
      "INFO:tensorflow:loss = -0.9410826, step = 10400 (0.915 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.496\n",
      "INFO:tensorflow:loss = -0.9266468, step = 10500 (0.922 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.972\n",
      "INFO:tensorflow:loss = -0.88954633, step = 10600 (0.944 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.35\n",
      "INFO:tensorflow:loss = -0.9074714, step = 10700 (0.923 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.282\n",
      "INFO:tensorflow:loss = -0.86025625, step = 10800 (0.932 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.982\n",
      "INFO:tensorflow:loss = -0.9058209, step = 10900 (0.902 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 11000...\n",
      "INFO:tensorflow:Saving checkpoints for 11000 into ranking_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 11000...\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (15 secs).\n",
      "INFO:tensorflow:global_step/sec: 49.5412\n",
      "INFO:tensorflow:loss = -0.9037293, step = 11000 (2.018 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.154\n",
      "INFO:tensorflow:loss = -0.8936831, step = 11100 (0.933 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.095\n",
      "INFO:tensorflow:loss = -0.911092, step = 11200 (0.934 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.606\n",
      "INFO:tensorflow:loss = -0.9150401, step = 11300 (0.920 sec)\n",
      "INFO:tensorflow:global_step/sec: 109.065\n",
      "INFO:tensorflow:loss = -0.9106477, step = 11400 (0.917 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.259\n",
      "INFO:tensorflow:loss = -0.9278541, step = 11500 (0.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.551\n",
      "INFO:tensorflow:loss = -0.89628947, step = 11600 (0.930 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.469\n",
      "INFO:tensorflow:loss = -0.9150184, step = 11700 (0.930 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.942\n",
      "INFO:tensorflow:loss = -0.8967418, step = 11800 (0.935 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.369\n",
      "INFO:tensorflow:loss = -0.87782085, step = 11900 (0.923 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 12000...\n",
      "INFO:tensorflow:Saving checkpoints for 12000 into ranking_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 12000...\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-11-10T21:43:19\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ranking_model_dir/model.ckpt-12000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-10 21:43:19.799589: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Inference Time : 0.75519s\n",
      "INFO:tensorflow:Finished evaluation at 2021-11-10-21:43:20\n",
      "INFO:tensorflow:Saving dict for global step 12000: global_step = 12000, labels_mean = 1.9630322, logits_mean = 1.2833068, loss = -0.86475474, metric/ndcg@1 = 0.6528571, metric/ndcg@10 = 0.8289143, metric/ndcg@3 = 0.7119563, metric/ndcg@5 = 0.76168084\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 12000: ranking_model_dir/model.ckpt-12000\n",
      "INFO:tensorflow:global_step/sec: 23.897\n",
      "INFO:tensorflow:loss = -0.8849029, step = 12000 (4.185 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.8\n",
      "INFO:tensorflow:loss = -0.9089664, step = 12100 (0.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.791\n",
      "INFO:tensorflow:loss = -0.9181893, step = 12200 (0.929 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.918\n",
      "INFO:tensorflow:loss = -0.88065004, step = 12300 (0.953 sec)\n",
      "INFO:tensorflow:global_step/sec: 104\n",
      "INFO:tensorflow:loss = -0.93088675, step = 12400 (0.961 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.862\n",
      "INFO:tensorflow:loss = -0.9150947, step = 12500 (0.954 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.407\n",
      "INFO:tensorflow:loss = -0.8844799, step = 12600 (0.967 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.9915\n",
      "INFO:tensorflow:loss = -0.915141, step = 12700 (1.000 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.385\n",
      "INFO:tensorflow:loss = -0.9095146, step = 12800 (0.967 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.025\n",
      "INFO:tensorflow:loss = -0.9169079, step = 12900 (0.935 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 13000...\n",
      "INFO:tensorflow:Saving checkpoints for 13000 into ranking_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 13000...\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (15 secs).\n",
      "INFO:tensorflow:global_step/sec: 47.6536\n",
      "INFO:tensorflow:loss = -0.9008373, step = 13000 (2.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.018\n",
      "INFO:tensorflow:loss = -0.9293176, step = 13100 (0.909 sec)\n",
      "INFO:tensorflow:global_step/sec: 112.504\n",
      "INFO:tensorflow:loss = -0.8977344, step = 13200 (0.889 sec)\n",
      "INFO:tensorflow:global_step/sec: 113.303\n",
      "INFO:tensorflow:loss = -0.9074253, step = 13300 (0.883 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.613\n",
      "INFO:tensorflow:loss = -0.8951708, step = 13400 (0.938 sec)\n",
      "INFO:tensorflow:global_step/sec: 109.094\n",
      "INFO:tensorflow:loss = -0.89028525, step = 13500 (0.917 sec)\n",
      "INFO:tensorflow:global_step/sec: 115.794\n",
      "INFO:tensorflow:loss = -0.8654176, step = 13600 (0.864 sec)\n",
      "INFO:tensorflow:global_step/sec: 111.042\n",
      "INFO:tensorflow:loss = -0.91601217, step = 13700 (0.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 109.525\n",
      "INFO:tensorflow:loss = -0.88039595, step = 13800 (0.913 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.392\n",
      "INFO:tensorflow:loss = -0.89566976, step = 13900 (0.949 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 14000...\n",
      "INFO:tensorflow:Saving checkpoints for 14000 into ranking_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 14000...\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-11-10T21:43:42\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ranking_model_dir/model.ckpt-14000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-10 21:43:42.836938: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Inference Time : 0.77553s\n",
      "INFO:tensorflow:Finished evaluation at 2021-11-10-21:43:43\n",
      "INFO:tensorflow:Saving dict for global step 14000: global_step = 14000, labels_mean = 1.9630322, logits_mean = 1.6237892, loss = -0.86332047, metric/ndcg@1 = 0.6528571, metric/ndcg@10 = 0.82731485, metric/ndcg@3 = 0.71397257, metric/ndcg@5 = 0.7591116\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 14000: ranking_model_dir/model.ckpt-14000\n",
      "INFO:tensorflow:global_step/sec: 23.6126\n",
      "INFO:tensorflow:loss = -0.9231356, step = 14000 (4.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.316\n",
      "INFO:tensorflow:loss = -0.8995377, step = 14100 (0.908 sec)\n",
      "INFO:tensorflow:global_step/sec: 109.616\n",
      "INFO:tensorflow:loss = -0.9285557, step = 14200 (0.911 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.432\n",
      "INFO:tensorflow:loss = -0.9120207, step = 14300 (0.922 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.738\n",
      "INFO:tensorflow:loss = -0.92924774, step = 14400 (0.928 sec)\n",
      "INFO:tensorflow:global_step/sec: 109.753\n",
      "INFO:tensorflow:loss = -0.9133907, step = 14500 (0.911 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.748\n",
      "INFO:tensorflow:loss = -0.92047834, step = 14600 (0.928 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.349\n",
      "INFO:tensorflow:loss = -0.9047891, step = 14700 (0.923 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.231\n",
      "INFO:tensorflow:loss = -0.92027044, step = 14800 (0.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 109.589\n",
      "INFO:tensorflow:loss = -0.8918661, step = 14900 (0.912 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 15000...\n",
      "INFO:tensorflow:Saving checkpoints for 15000 into ranking_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 15000...\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (15 secs).\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-11-10T21:43:55\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ranking_model_dir/model.ckpt-15000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-10 21:43:55.378840: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Inference Time : 0.77847s\n",
      "INFO:tensorflow:Finished evaluation at 2021-11-10-21:43:56\n",
      "INFO:tensorflow:Saving dict for global step 15000: global_step = 15000, labels_mean = 1.9630322, logits_mean = 1.572945, loss = -0.863917, metric/ndcg@1 = 0.64428574, metric/ndcg@10 = 0.8276546, metric/ndcg@3 = 0.7203656, metric/ndcg@5 = 0.76241815\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 15000: ranking_model_dir/model.ckpt-15000\n",
      "INFO:tensorflow:Loss for final step: -0.90340865.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'labels_mean': 1.9630322,\n",
       "  'logits_mean': 1.572945,\n",
       "  'loss': -0.863917,\n",
       "  'metric/ndcg@1': 0.64428574,\n",
       "  'metric/ndcg@10': 0.8276546,\n",
       "  'metric/ndcg@3': 0.7203656,\n",
       "  'metric/ndcg@5': 0.76241815,\n",
       "  'global_step': 15000},\n",
       " [])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker, train_spec, eval_spec = train_and_eval_fn()\n",
    "tf.estimator.train_and_evaluate(ranker, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4200cc",
   "metadata": {},
   "source": [
    "### Launch TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ed9dfa98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T19:11:38.898375Z",
     "start_time": "2021-11-10T19:11:38.718736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ERROR: Could not find `tensorboard`. Please ensure that your PATH\n",
       "contains an executable `tensorboard` program, or explicitly specify\n",
       "the path to a TensorBoard binary by setting the `TENSORBOARD_BINARY`\n",
       "environment variable."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=\"ranking_model_dir\" --port 12345"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791a1841",
   "metadata": {},
   "source": [
    "### Generating Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aa7489",
   "metadata": {},
   "source": [
    "Мы показываем, как создавать прогнозы по функциям набора данных. Мы предполагаем, что метка отсутствует и должна быть выведена с использованием модели ранжирования.\n",
    "\n",
    "Подобно input_fn, используемому для обучения и оценки, pred_input_fn считывает данные в формате ELWC и сохраняет их как TFRecords для создания функций. Мы устанавливаем количество эпох равным 1, чтобы генератор прекратил итерацию, когда достигнет конца набора данных. Кроме того, при чтении точки данных не перемешиваются, поэтому поведение функции predic () является детерминированным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d6fe3536",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T19:13:30.820624Z",
     "start_time": "2021-11-10T19:13:30.817076Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_input_fn(path):\n",
    "    context_feature_spec = tf.feature_column.make_parse_example_spec(\n",
    "        context_feature_columns().values())\n",
    "    example_feature_spec = tf.feature_column.make_parse_example_spec(\n",
    "        list(example_feature_columns().values()))\n",
    "    dataset = tfr.data.build_ranking_dataset(\n",
    "        file_pattern=path,\n",
    "        data_format=tfr.data.ELWC,\n",
    "        batch_size=_BATCH_SIZE,\n",
    "        list_size=_LIST_SIZE,\n",
    "        context_feature_spec=context_feature_spec,\n",
    "        example_feature_spec=example_feature_spec,\n",
    "        reader=tf.data.TFRecordDataset,\n",
    "        shuffle=False,\n",
    "        num_epochs=1)\n",
    "    features = tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e72b802",
   "metadata": {},
   "source": [
    "Будем генерировать прогнозы на тестовом наборе данных, где мы рассматриваем только контекст и примеры функций и прогнозируем метки. Pred_input_fn генерирует прогнозы для пакета точек данных. Пакетная обработка позволяет нам перебирать большие наборы данных, которые невозможно загрузить в память."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d092c2b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T19:14:08.768202Z",
     "start_time": "2021-11-10T19:14:08.765707Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = ranker.predict(input_fn=lambda: predict_input_fn(\"test.tfrecords\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66c8f01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T19:14:38.106733Z",
     "start_time": "2021-11-10T19:14:38.103679Z"
    }
   },
   "source": [
    "ranker.predict возвращает генератор, который мы можем перебирать для создания прогнозов, пока генератор не будет исчерпан."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "47f829ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T19:14:51.659429Z",
     "start_time": "2021-11-10T19:14:50.570231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "WARNING:tensorflow:Input graph does not use tf.data.Dataset or contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in query_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file vocab.txt.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ranking_model_dir/model.ckpt-15000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-10 22:14:51.390680: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "x = next(predictions)\n",
    "assert len(x) == _LIST_SIZE  # Note that this includes padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d661bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MTS ED",
   "language": "python",
   "name": "shad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
